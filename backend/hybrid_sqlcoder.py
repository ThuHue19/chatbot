import os
import re
import logging
import json
import unidecode
from typing import List, Dict, Optional
from dotenv import load_dotenv
from difflib import get_close_matches
from rapidfuzz import process, fuzz

from utils.sql_planner import SQLPlanner
from utils.column_mapper import generate_column_mapping_hint
from utils.schema_loader import TABLE_KEYWORDS, extract_relevant_tables
from utils.relation_loader import load_relations
from db_utils import get_connection, execute_sql, load_locality_cache_json, load_dept_cache_json


# --- Setup m√¥i tr∆∞·ªùng & logger ---
load_dotenv()
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)
LOCALITY_CACHE = load_locality_cache_json()
DEPT_CACHE_FILE = load_dept_cache_json()

logger.info(f"‚úÖ ƒê√£ load {len(LOCALITY_CACHE)} b·∫£n ghi ƒë·ªãa ch·ªâ t·ª´ cache.")
logger.info(f"‚úÖ ƒê√£ load {len(DEPT_CACHE_FILE)} b·∫£n ghi t·ªï nh√≥m, ph√≤ng ban t·ª´ cache.")

LLM_ENGINE = os.getenv("LLM_ENGINE", "openai").lower()
conn = get_connection()
def get_ma_dia_chi_from_cache(dia_chi_text: str) -> dict:
    """
    Map ƒë·ªãa ch·ªâ user nh·∫≠p sang m√£ t·ªânh/qu·∫≠n d·ª±a tr√™n LOCALITY_CACHE.
    """
    if not dia_chi_text:
        return {}

    # Chu·∫©n h√≥a v√† t√°ch c√°c t·ª´ kh√≥a
    tokens = [t.strip() for t in re.split(r"[,\n]", dia_chi_text) if t.strip()]
    for token in tokens:
        if token in LOCALITY_CACHE:
            item = LOCALITY_CACHE[token]
            return {
                "tinhThanhPho": item.get("province_code", ""),
                "quanHuyen": item.get("district_code", "")
            }
    # N·∫øu kh√¥ng t√¨m th·∫•y
    return {}

dept_keywords = ["t·ªï", "nh√≥m", "ph√≤ng", "ban", "t·ªï nh√≥m", "ph√≤ng ban","ƒë√†i"]
canhan_keywords = ["c√° nh√¢n", "nh√¢n vi√™n", "ng∆∞·ªùi x·ª≠ l√Ω", "anh", "c√¥"]

def summarize_rows(columns, rows, max_rows=20):
    """
    N·∫øu rows nhi·ªÅu h∆°n max_rows th√¨ tr·∫£ v·ªÅ th·ªëng k√™/t√≥m t·∫Øt,
    ng∆∞·ª£c l·∫°i tr·∫£ v·ªÅ full rows.
    """
    if len(rows) <= max_rows:
        return {"columns": columns, "rows": rows}

    # T√≥m t·∫Øt: ƒë·∫øm s·ªë b·∫£n ghi, l·∫•y v√†i v√≠ d·ª•
    summary = {
        "total_records": len(rows),
        "sample_rows": [dict(zip(columns, r)) for r in rows[:3]]
    }

    # N·∫øu c√≥ c·ªôt CA_NHAN / TO_NHOM ‚Üí gom nh√≥m lu√¥n
    if "CA_NHAN" in columns:
        idx = columns.index("CA_NHAN")
        counter = {}
        for r in rows:
            key = r[idx]
            counter[key] = counter.get(key, 0) + 1
        summary["count_by_canhan"] = counter

    if "TO_NHOM" in columns:
        idx = columns.index("TO_NHOM")
        counter = {}
        for r in rows:
            key = r[idx]
            counter[key] = counter.get(key, 0) + 1
        summary["count_by_tonhom"] = counter

    return {"summary": summary}

# H√†m ki·ªÉm tra xu·∫•t hi·ªán ch√≠nh x√°c
def contains_keyword(text: str, keywords: list) -> bool:
    text_lower = text.lower()
    for kw in keywords:
        # \b ƒë·ªÉ match nguy√™n t·ª´, kh√¥ng nh·∫≠n c√°c t·ª´ ch·ª©a
        if re.search(rf"\b{re.escape(kw)}\b", text_lower):
            return True
    return False
FORM_DETAIL_PARAMS = [
    "type", "trungTam", "phongBan", "toNhom", "caNhan", "loaiPa", "loaiThueBao",
    "nguyenNhan", "caTruc", "fo_bo", "tinh", "ctdv", "day", "today", "week",
    "month", "quarter", "year", "kpiName", "quanHuyen", "level", "nhomNguyenNhan"
]
LIST_PARAMS = [
    "tab", "id", "year", "tinhThanhPho", "quanHuyen", "is_ticket", "phong", "to", "caTruc",
    "dept_xuly", "dept", "ttTicket", "nhomPa", "loaiPa", "fo_bo", "loaiThueBao", "is_dongpa", "ctdv"
]
def normalize_canhan(value: str) -> str:
    """Chu·∫©n h√≥a t√™n ƒë·ªÉ so s√°nh fuzzy (b·ªè d·∫•u, b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát, vi·∫øt li·ªÅn)"""
    if not value:
        return ""
    # B·ªè d·∫•u, chuy·ªÉn th∆∞·ªùng
    value = unidecode.unidecode(str(value)).lower()
    # X√≥a k√Ω t·ª± kh√¥ng ph·∫£i ch·ªØ/s·ªë
    value = re.sub(r'[^a-z0-9]', '', value)
    return value

def fuzzy_match_canhan(user_input: str, db_values: List[str], threshold: float = 70) -> Optional[str]:
    """
    Fuzzy match t√™n c√° nh√¢n t·ª´ user_input v·ªõi danh s√°ch ca_nhan (db_values).
    Tr·∫£ v·ªÅ chu·ªói g·ªëc n·∫øu t√¨m ƒë∆∞·ª£c match.
    threshold: % ƒë·ªô t∆∞∆°ng ƒë·ªìng t·ªëi thi·ªÉu (0-100)
    """
    if not user_input or not db_values:
        return None

    norm_input = normalize_canhan(user_input)

    # Map normalized -> original
    norm_map = {normalize_canhan(v): v for v in db_values}

    # T√≠nh score t·ª´ng ·ª©ng vi√™n
    candidates = []
    for norm_val, orig in norm_map.items():
        score = fuzz.partial_ratio(norm_input, norm_val)
        candidates.append((orig, score))

    # L·∫•y match t·ªët nh·∫•t
    best_match = max(candidates, key=lambda x: x[1], default=(None, 0))
    if best_match[1] >= threshold:
        return best_match[0]
    return None
def resolve_canhan(user_input: str) -> Optional[str]:
    try:
        with get_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute("SELECT DISTINCT ca_nhan FROM PAKH_SLA_CA_NHAN")
                ca_nhans = [row[0] for row in cursor.fetchall()]
                match = fuzzy_match_canhan(user_input, ca_nhans)
                
                if match:
                    logger.info(f"üë§ Fuzzy match c√° nh√¢n: '{user_input}' ‚Üí '{match}'")
                return match
    except Exception as e:
        logger.error(f"L·ªói trong resolve_canhan: {e}")
        return None
import unicodedata
def normalize_text(text: str) -> str:
    text = text.lower()
    text = unicodedata.normalize("NFD", text)  # b·ªè d·∫•u
    text = re.sub(r"[\u0300-\u036f]", "", text)
    text = re.sub(r"[^a-z0-9\s]", " ", text)
    return re.sub(r"\s+", " ", text).strip()
def load_dept_cache_json_separated():
    all_depts = load_dept_cache_json()  # list dict
    to_nhom_list = [d for d in all_depts if "TVT" in d["dept_name"] or "TVT" in d["dept_code"].lower()]
    phong_ban_list = [d for d in all_depts if d not in to_nhom_list]
    return to_nhom_list, phong_ban_list

def fuzzy_match_department(user_input: str, kind: str = "all", cutoff: float = 0.7):
    """
    kind = "to_nhom", "phong_ban", ho·∫∑c "all"
    """
    to_nhom_list, phong_ban_list = load_dept_cache_json_separated()

    if kind == "to_nhom":
        departments = to_nhom_list
    elif kind == "phong_ban":
        departments = phong_ban_list
    else:
        departments = to_nhom_list + phong_ban_list

    if not departments:
        return None, None, 0

    norm_input = normalize_text(user_input)
    dept_map = {normalize_text(d["dept_name"]): (d["dept_code"], d["dept_name"]) for d in departments}

    best_match, score, _ = process.extractOne(norm_input, dept_map.keys(), scorer=fuzz.WRatio)
    if score >= cutoff * 100:
        dept_code, dept_name = dept_map[best_match]
        return dept_code, dept_name, score
    return None, None, 0

def load_known_places(db_conn):
    cursor = db_conn.cursor()
    cursor.execute("SELECT full_name FROM FB_LOCALITY")
    rows = cursor.fetchall()
    return {normalize_text(r[0]) for r in rows if r[0]}
    
def _clean_sql_for_param_extraction(sql: str) -> str:
    """Lo·∫°i b·ªè LOWER(), UPPER(), TRIM()... quanh t√™n c·ªôt ƒë·ªÉ regex b·∫Øt d·ªÖ h∆°n"""
    sql = str(sql or "")
    sql_clean = re.sub(r"\bLOWER\s*\(\s*([A-Z_]+)\s*\)", r"\1", sql, flags=re.IGNORECASE)
    sql_clean = re.sub(r"\bUPPER\s*\(\s*([A-Z_]+)\s*\)", r"\1", sql_clean, flags=re.IGNORECASE)
    sql_clean = re.sub(r"\bTRIM\s*\(\s*([A-Z_]+)\s*\)", r"\1", sql_clean, flags=re.IGNORECASE)
    return sql_clean

def extract_form_detail_params_from_sql(sql: str, context: dict = None, get_ma_dia_chi_fuzzy=None) -> dict:
    sql = _clean_sql_for_param_extraction(sql)
    default_values = {k: "" for k in FORM_DETAIL_PARAMS}
    default_values.update({
        "type": "year",
        "today": "undefined",
        "week": "undefined",
        "month": "undefined",
        "quarter": "undefined",
    })
    params = default_values.copy()

    # --- Extract b·∫±ng regex b√¨nh th∆∞·ªùng ---
    regex_map = {
        "trungTam": r"TRUNG_TAM\s*=\s*'([^']+)'",
        "phongBan": r"PHONG_BAN\s*=\s*'([^']+)'",
        "toNhom": r"TO_NHOM\s*=\s*'([^']+)'",
        "caNhan": r"(?:\b\w+\.)?CA_NHAN\s*=\s*'([^']+)'",
        "loaiPa": r"LOAI_PA\s*=\s*'([^']+)'",
        "loaiThueBao": r"LOAI_THUE_BAO\s*=\s*'([^']+)'",
        "nguyenNhan": r"NGUYEN_NHAN\s*=\s*'([^']+)'",
        "caTruc": r"CA_TRUC\s*=\s*'([^']+)'",
        "fo_bo": r"FO_BO\s*=\s*'([^']+)'",
        "tinh": r"TINH\s*=\s*'([^']+)'",
        "ctdv": r"CTDV\s*=\s*'([^']+)'",
        "day": r"DAY\s*=\s*'([^']+)'",
        "week": r"WEEK\s*=\s*'([^']+)'",
        "month": r"MONTH\s*=\s*'([^']+)'",
        "quarter": r"QUARTER\s*=\s*'([^']+)'",
        "year": r"YEAR\s*=\s*'([^']+)'",
        "kpiName": r"KPI_NAME\s*=\s*'([^']+)'",
        "quanHuyen": r"QUAN_HUYEN\s*=\s*'([^']+)'",
        "level": r"LEVEL\s*=\s*'([^']+)'",
        "nhomNguyenNhan": r"NHOM_NGUYEN_NHAN\s*=\s*'([^']+)'",
    }
    for key, regex in regex_map.items():
        match = re.search(regex, sql, re.IGNORECASE)
        if match:
            params[key] = match.group(1)

    # --- Override b·∫±ng context n·∫øu c√≥ ---
    if context:
        for key in params:
            if key in context and context[key]:
                params[key] = context[key]

    # --- X√°c ƒë·ªãnh level ---
    sql_upper = sql.upper()
    if not params["level"]:
        if "PAKH_CA_NHAN" in sql_upper:
            params["level"] = "ca_nhan"
        elif "PAKH_TO_NHOM" in sql_upper:
            params["level"] = "to_nhom"
        elif "PAKH_PHONG_BAN" in sql_upper:
            params["level"] = "phong_ban"
        elif "PAKH_TRUNG_TAM" in sql_upper:
            params["level"] = "trung_tam"

    # --- X√°c ƒë·ªãnh nƒÉm ---
    if not params["year"]:
        match = re.search(r"\b(20[2-3][0-9])\b", sql)
        if match:
            params["year"] = match.group(1)
        elif context and "year" in context and context["year"]:
            params["year"] = context["year"]

    # --- X√°c ƒë·ªãnh KPI ---
    if not params["kpiName"]:
        trang_thai_map = {
            "TU_CHOI": "soPaTuChoi",
            "DONG": "soPaDong",
            "DANG_XU_LY": "soPaDangXlDh",
            "DA_XU_LY": "soPaDaXlyDh",
            "HOAN_THANH": "soPaHoanThanh",
        }
        match = re.search(r"TRANG_THAI\s*=\s*'([^']+)'", sql, re.IGNORECASE)
        if match:
            trang_thai = match.group(1).upper()
            if trang_thai in trang_thai_map:
                params["kpiName"] = trang_thai_map[trang_thai]

    # --- X√°c ƒë·ªãnh caTruc ---
    if not params["caTruc"]:
        ca_truc_map = {"SANG": "SANG", "CHIEU": "CHIEU", "TOI": "TOI"}
        match = re.search(r"CA_TRUC\s*=\s*'([^']+)'", sql, re.IGNORECASE)
        if match:
            ca_truc = match.group(1).upper()
            if ca_truc in ca_truc_map:
                params["caTruc"] = ca_truc_map[ca_truc]

    # --- Lo·∫°i thu√™ bao ---
    if not params["loaiThueBao"]:
        loai_thue_bao_map = {"KHDN": "KHDN", "KHCN": "KHCN", "VIP": "VIP"}
        match = re.search(r"LOAI_THUE_BAO\s*=\s*'([^']+)'", sql, re.IGNORECASE)
        if match:
            loai_thue_bao = match.group(1).upper()
            if loai_thue_bao in loai_thue_bao_map:
                params["loaiThueBao"] = loai_thue_bao_map[loai_thue_bao]

    return params


def extract_list_params_from_sql(sql: str, context: dict = None, get_ma_dia_chi_fuzzy=None) -> dict:
    sql = _clean_sql_for_param_extraction(sql)
    default_values = {k: "" for k in LIST_PARAMS}
    default_values.update({
        "tab": "year",
        "is_ticket": "Y%2CN",
        "is_dongpa": "DANG_XU_LY%2CDA_XU_LY",
    })
    params = default_values.copy()

    # --- Extract b·∫±ng regex ---
    regex_map = {
        "tab": r"tab\s*=\s*'([^']+)'",
        "id": r"id\s*=\s*'([^']+)'",
        "year": r"year\s*=\s*'([^']+)'",
        "tinhThanhPho": r"TINH_THANH_PHO\s*=\s*'([^']+)'",
        "quanHuyen": r"QUAN_HUYEN\s*=\s*'([^']+)'",
        "is_ticket": r"is_ticket\s*=\s*'([^']+)'",
        "phong": r"PHONG\s*=\s*'([^']+)'",
        "to": r"TO_NHOM\s*=\s*'([^']+)'",
        "caTruc": r"CA_TRUC\s*=\s*'([^']+)'",
        "dept_xuly": r"DEPT_XULY\s*=\s*'([^']+)'",
        "dept": r"DEPT\s*=\s*'([^']+)'",
        "ttTicket": r"TT_TICKET\s*=\s*'([^']+)'",
        "nhomPa": r"NHOM_PA\s*=\s*'([^']+)'",
        "loaiPa": r"LOAI_PA\s*=\s*'([^']+)'",
        "fo_bo": r"FO_BO\s*=\s*'([^']+)'",
        "loaiThueBao": r"LOAI_THUE_BAO\s*=\s*'([^']+)'",
        "is_dongpa": r"is_dongpa\s*=\s*'([^']+)'",
        "ctdv": r"CTDV\s*=\s*'([^']+)'"
    }
    for key, regex in regex_map.items():
        match = re.search(regex, sql, re.IGNORECASE)
        if match:
            params[key] = match.group(1)

    
    # --- Override b·∫±ng context n·∫øu c√≥ ---
    if context:
        for key in params:
            if key in context and context[key]:
                params[key] = context[key]

    return params


def build_filter_url(base: str, params: Dict[str, str]) -> str:
    query = "&".join(f"{k}={v}" for k, v in params.items())
    return f"{base}?{query}"

# --- L·ªõp Cache cho Schema ---
class SchemaCache:
    def __init__(self):
        self.cache = {}
    def get_schema(self, table_name):
        if table_name not in self.cache:
            from utils.schema_loader import load_schema
            self.cache[table_name] = load_schema(table_name)
        return self.cache[table_name]
    def clear(self):
        self.cache.clear()
        logger.info("ƒê√£ x√≥a schema cache.")
        
# --- L·ªõp ch√≠nh HybridSQLCoder ---
class HybridSQLCoder:
    def __init__(self, db_conn=None):
        self.db_conn = db_conn
        self.engine = LLM_ENGINE
        self.sql_cache = {}
        self.schema_cache = SchemaCache()
        self.context = {}
        self.context_filters = None
        self.known_places = load_known_places(self.db_conn) if self.db_conn else set()
        self.relations, self.concat_mappings = load_relations()
        self.context = {
            "filters": {},
            "subjects": set(),
            "last_question": None,
            "last_sql": None,
            "last_result": None
        }
        self.db_conn = db_conn
        BASE_DIR = os.path.dirname(os.path.abspath(__file__))
        EXAMPLES_FILE = os.path.join(BASE_DIR, "utils", "sql_examples.json")
        self.sql_examples = self._load_sql_examples(EXAMPLES_FILE)

        if self.engine == "openai":
            self._init_openai()
        else:
            self._init_gemini()

    def _init_openai(self):
        from langchain_openai import ChatOpenAI
        from langchain.memory import ConversationSummaryMemory

        self.model_name = os.getenv("OPENAI_MODEL", "gpt-4")
        self.llm = ChatOpenAI(model=self.model_name, temperature=0)
        self.memory = ConversationSummaryMemory(
            llm=self.llm, memory_key="chat_history", return_messages=True
        )

    def _init_gemini(self):
        import google.generativeai as genai

        api_key = os.getenv("GEMINI_API_KEY")
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel("models/gemini-2.5-flash", generation_config={"temperature": 0})
        self.memory = []  # list of (user_message, ai_message)

    
    def _fallback_to_gemini(self):
        logger.warning("OpenAI g·∫∑p l·ªói, chuy·ªÉn sang Gemini.")
        self._init_gemini()
        self.engine = "gemini"
    def _extract_where_conditions(self, sql: str) -> str:
        """Tr√≠ch xu·∫•t ph·∫ßn ƒëi·ªÅu ki·ªán WHERE t·ª´ c√¢u SQL (kh√¥ng g·ªìm ch·ªØ WHERE)."""
        sql = str(sql or "").strip().rstrip(";")
        match = re.search(r"\bWHERE\b\s+(.*?)(?:\bGROUP\b|\bORDER\b|$)", sql, re.IGNORECASE | re.DOTALL)
        if match:
            return match.group(1).strip()
        return ""
    # --- Helper ---
    def _get_last_n_user_questions(self, n=5):
        if self.engine == "openai" and hasattr(self.memory, "chat_memory"):
            return [
                msg.content for msg in reversed(self.memory.chat_memory.messages)
                if getattr(msg, "type", "") == "human"
            ][:n][::-1]
        elif self.engine == "gemini":
            return [user for user, _ in self.memory[-n:]][::-1]
        return []

    def reset_context(self):
        self.context = {
            "filters": {},
            "subjects": set(),
            "last_question": None,
            "last_sql": None,
            "last_result": None
        }
        logger.info("[CONTEXT] ƒê√£ reset to√†n b·ªô context.")
    def update_context(self, sql=None, result=None, question=None):
        """C·∫≠p nh·∫≠t context t·ª´ SQL, k·∫øt qu·∫£ truy v·∫•n v√† c√¢u h·ªèi."""
        if question is not None:
            self.context["last_question"] = question
        if sql is not None:
            self.context["last_sql"] = sql
        if result is not None:
            self.context["last_result"] = result
    def encode(self, text: str) -> str:
        return str(normalize_text(text) or "")
    def get_ma_dia_chi_fuzzy(self, dia_chi_text: str) -> dict:
        """Map ƒë·ªãa ch·ªâ user nh·∫≠p sang m√£ t·ªânh/qu·∫≠n trong DB."""
        if not self.db_conn or not dia_chi_text:
            return {}

        norm_text = normalize_text(dia_chi_text)
        # t√°ch theo d·∫•u ph·∫©y ho·∫∑c kho·∫£ng tr·∫Øng
        parts = [p.strip() for p in re.split(r"[,\n]", dia_chi_text) if p.strip()]

        # Load danh s√°ch FULL_NAME t·ª´ DB (c√≥ th·ªÉ cache ƒë·ªÉ t·ªëi ∆∞u)
        cursor = self.db_conn.cursor()
        cursor.execute("SELECT province, district, full_name FROM FB_LOCALITY")
        rows = cursor.fetchall()
        cursor.close()

        best_match = {"tinhThanhPho": None, "quanHuyen": None}
        max_score = 0

        for province, district, full_name in rows:
            norm_full = normalize_text(full_name)
            # score = s·ªë t·ª´ tr√πng kh·ªõp gi·ªØa input v√† full_name
            input_tokens = set(norm_text.split())
            full_tokens = set(norm_full.split())
            score = len(input_tokens & full_tokens) / max(len(full_tokens), 1)
            if score > max_score:
                max_score = score
                best_match["tinhThanhPho"] = province
                best_match["quanHuyen"] = district

        if max_score < 0.3:  # n·∫øu qu√° √≠t token tr√πng, coi nh∆∞ kh√¥ng t√¨m th·∫•y
            return {}

        return best_match



    from difflib import get_close_matches

    @staticmethod
    def fuzzy_match_diachi(user_input: str, db_fullnames: List[str], cutoff=0.8):
        """Map ƒë·ªãa ch·ªâ user nh·∫≠p sang FULL_NAME trong DB (static method tr√°nh t·ª± b∆°m self)."""
        norm_input = normalize_text(user_input)
        norm_map = {normalize_text(v): v for v in db_fullnames}
        matches = get_close_matches(norm_input, norm_map.keys(), n=1, cutoff=cutoff)
        if matches:
            return norm_map[matches[0]]
        return None

    def _resolve_follow_up_question(self, question: str, is_follow_up: bool = False) -> str:
        if not is_follow_up:
            return question
        last_q = self.context["last_question"] or ""
        if not last_q:
            return question
        prompt = f"""
B·∫°n l√† tr·ª£ l√Ω ng√¥n ng·ªØ.
C√¢u h·ªèi tr∆∞·ªõc: {last_q}
C√¢u h·ªèi hi·ªán t·∫°i: {question}
H√£y vi·∫øt l·∫°i c√¢u h·ªèi hi·ªán t·∫°i th√†nh c√¢u h·ªèi ƒë·∫ßy ƒë·ªß, r√µ nghƒ©a.
"""
        resolved = self._invoke_model(prompt)
        return resolved.strip() if resolved else question

    def _format_relations_for_prompt(self):
        return os.linesep.join(
            f"- {table}.{col} ‚Üí {ref['ref_table']}.{ref['ref_column']}"
            for table, cols in self.relations.items()
            for col, ref in cols.items()
        )

    def _generate_sum_hint(self) -> str:
        hints = [
            "- ƒê·ªëi v·ªõi c√°c b·∫£ng SLA (PAKH_SLA_*):",
            "   - Lu√¥n d√πng SUM() v·ªõi c√°c c·ªôt t·ªïng h·ª£p nh∆∞ SO_PA_NHAN, SO_PA_DA_XL, SO_PA_QH.",
            "   - √Ånh x·∫° t·ª´ kh√≥a trong c√¢u h·ªèi sang c·ªôt t·ªïng h·ª£p:",
            "     - 'qu√° h·∫°n' ‚Üí SUM(SO_PA_QH) (S·ªë ph·∫£n √°nh qu√° h·∫°n)",
            "     - 'ƒë√£ x·ª≠ l√Ω ƒë√∫ng h·∫°n' ‚Üí SUM(SO_PA_DA_XL_DH)",
            "     - 'ƒëang x·ª≠ l√Ω ƒë√∫ng h·∫°n' ‚Üí SUM(SO_PA_DANG_XL_DH)",
            "     - 't·ª´ ch·ªëi' ho·∫∑c 'b·ªã t·ª´ ch·ªëi' ‚Üí SUM(SO_PA_TU_CHOI)",
            "     - 't·ªïng th·ªùi gian x·ª≠ l√Ω' ‚Üí SUM(TONG_TG_XL) / 60 (ƒë·ªïi sang gi·ªù)",
            "     - 'th·ªùi gian trung b√¨nh x·ª≠ l√Ω' ‚Üí  ROUND(SUM(TONG_TG_XL) / NULLIF(SUM(SO_PA_DA_XL), 0), 2) AS TB_PHUT, TRUNC(SUM(TONG_TG_XL) / NULLIF(SUM(SO_PA_DA_XL), 0) / 60) AS GIO, MOD(ROUND(SUM(TONG_TG_XL) / NULLIF(SUM(SO_PA_DA_XL), 0)), 60) AS PHUT",
            "   - TUY·ªÜT ƒê·ªêI KH√îNG JOIN b·∫£ng SLA tr·ª±c ti·∫øp v·ªõi PAKH ho·∫∑c PAKH_CA_NHAN cho c√°c truy v·∫•n th·ªëng k√™.",
            "- C√¥ng th·ª©c t√≠nh t·ª∑ l·ªá x·ª≠ l√Ω ƒë√∫ng h·∫°n: (SUM(SO_PA_DA_XL_DH) + SUM(SO_PA_DANG_XL_DH)) / NULLIF(SUM(SO_PA_NHAN), 0) * 100.",
            "- D√πng b·∫£ng SLA ph√π h·ª£p v·ªõi ng·ªØ c·∫£nh c√° nh√¢n, t·ªï nh√≥m, ph√≤ng ban, trung t√¢m",
            "- L·∫•y t√™n c√° nh√¢n trong c·ªôt CA_NHAN"
        ]
        return os.linesep.join(hints)
    def _load_sql_examples(self, file_path: str) -> List[Dict[str, str]]:
        if os.path.exists(file_path):
            with open(file_path, "r", encoding="utf-8") as f:
                return json.load(f)
        logger.warning(f"File v√≠ d·ª• SQL kh√¥ng t√¨m th·∫•y: {file_path}")
        return []

    def _select_relevant_examples(self, question: str, num_examples: int = 3) -> str:
        question_lower = str(question or "").lower()
        question_words = set(question_lower.split())
        scored_examples = []
        for example in self.sql_examples or []:
            score = len(question_words.intersection(set(str(example.get("question", "")).lower().split())))
            if score > 0:
                scored_examples.append((score, example))
        scored_examples.sort(key=lambda x: x[0], reverse=True)
        relevant_examples = [ex for _, ex in scored_examples[:num_examples]]
        example_str = ""
        if relevant_examples:
            example_str += "\n---EXAMPLES START---\n"
            for ex in relevant_examples:
                example_str += f"C√¢u h·ªèi: {ex['question']}\nSQL:\n```sql\n{ex['sql']}\n```\n\n"
            example_str += "---EXAMPLES END---\n\n"
        return example_str
    # --- Sinh SQL ---
    def generate_sql(self, question: str, is_follow_up: bool = False, previous_error: str = None,
                     retries: int = 2, force_no_cache: bool = False, reset: bool = False):
        # Reset n·∫øu l√† c√¢u h·ªèi m·ªõi ƒë·ªôc l·∫≠p
        if reset and not is_follow_up:
            self.reset_context()

        # L∆∞u c√¢u h·ªèi ngay
        self.update_context(question=question)

        key = str(question or "").strip().lower()
        if self.context.get("subjects"):
            key += "__" + "__".join(sorted(str(sub).lower() for sub in self.context["subjects"]))

        # Cache
        if not force_no_cache and key in self.sql_cache:
            logger.info(f"[CACHE] S·ª≠ d·ª•ng SQL cache cho: {question}")
            return self.sql_cache[key]

        for attempt in range(retries):
            try:
                context_for_prompt = ""
                if is_follow_up and all(self.context.get(k) for k in ["last_question", "last_sql", "last_result"]):
                    filter_str = ""
                    if self.context["filters"]:
                        filter_str = "\nC√°c b·ªô l·ªçc ƒë√£ l∆∞u t·ª´ tr∆∞·ªõc: " + json.dumps(self.context["filters"], ensure_ascii=False)
                    context_for_prompt = f"""
# Ng·ªØ c·∫£nh h·ªôi tho·∫°i tr∆∞·ªõc ƒë√≥:
C√¢u h·ªèi: {self.context['last_question']}
SQL: {self.context['last_sql']}
K·∫øt qu·∫£: {json.dumps(self.context['last_result'], ensure_ascii=False)}
{filter_str}
C√¢u h·ªèi m·ªõi: {question}
# H∆∞·ªõng d·∫´n 
- N·∫øu ng∆∞·ªùi d√πng d√πng t·ª´ "ƒë√≥", "ti·∫øp", "li·ªát k√™ ƒëi", "th√™m chi ti·∫øt", h√£y coi ƒë√≥ l√† ti·∫øp n·ªëi c·ªßa c√¢u tr∆∞·ªõc v√† ph·∫£i l·ªçc d·ª±a tr√™n k·∫øt qu·∫£/SQL tr∆∞·ªõc.
- N·∫øu ng∆∞·ªùi d√πng nh·∫≠p c√¢u h·ªèi m·ªõi ƒë·∫ßy ƒë·ªß (c√≥ ƒë·ªß ch·ªß th·ªÉ, b·ªô l·ªçc),  h√£y coi l√† c√¢u ƒë·ªôc l·∫≠p.

"""

                question_resolved = self._resolve_follow_up_question(question, is_follow_up)
                planner = SQLPlanner(self._invoke_model)
                plan_result = planner.plan(question_resolved)
                relevant_tables = plan_result.get("tables", []) or extract_relevant_tables(question_resolved)

                relevant_examples_str = self._select_relevant_examples(question)
                if "FB_LOCALITY" not in relevant_tables:
                    relevant_tables.append("FB_LOCALITY")

                schema_text = os.linesep + os.linesep.join(
                    self.schema_cache.get_schema(tbl) for tbl in relevant_tables if tbl
                )

                column_mapping_hint = generate_column_mapping_hint(question_resolved)

                prompt_text = f"""
{context_for_prompt}
B·∫°n l√† tr·ª£ l√Ω sinh truy v·∫•n SQL cho CSDL Oracle.
# M·ª•c ti√™u
Sinh truy v·∫•n SQL ch√≠nh x√°c. ∆Øu ti√™n tr·∫£ l·ªùi nhanh ch√≥ng, ƒë∆°n gi·∫£n.

{relevant_examples_str}

# √Ånh x·∫° t·ª´ t·ª´ kh√≥a ‚Üí b·∫£ng.c·ªôt
{column_mapping_hint}
        
# SCHEMA
{schema_text}

# H∆∞·ªõng d·∫´n
{self._generate_sum_hint()}
- Ch·∫•p nh·∫≠n c√°c h√¨nh th·ª©c vi·∫øt t·∫Øt, v√≠ d·ª• KHCN - Kh√°ch h√†ng c√° nh√¢n, KHDN - Kh√°ch h√†ng doanh nghi·ªáp, pa hay p/a - ph·∫£n √°nh, HCM - Th√†nh ph·ªë H·ªì Ch√≠ Minh.
- V·ªõi s·ªë thu√™ bao ng∆∞·ªùi d√πng, trong c∆° s·ªü d·ªØ li·ªáu ƒëang c√≥ ƒë·ªãnh d·∫°ng kh√¥ng c√≥ s·ªë 0 ·ªü ƒë·∫ßu, khi ng∆∞·ªùi d√πng nh·∫≠p c√¢u h·ªèi B·∫ÆT BU·ªòC ch·∫•p nh·∫≠n c·∫£ ki·ªÉu nh·∫≠p C√ì S·ªê 0 v√† KH√îNG c√≥ s·ªë 0.
- V·ªõi nh·ªØng c√¢u li·ªát k√™, ch·ªâ tr·∫£ v·ªÅ c·ªôt PAKH.SO_THUE_BAO v√† PAKH.NOI_DUNG_PHAN_ANH, KH√îNG TR·∫¢ C√ÅC C·ªòT KH√ÅC.
- TUY·ªÜT ƒê·ªêI KH√îNG ƒë∆∞·ª£c truy v·∫•n tr·ª±c ti·∫øp c√°c c·ªôt ƒë·ªãa ch·ªâ nh∆∞ TINH_THANH_PHO, QUAN_HUYEN, PHUONG_XA b·∫±ng LIKE. Ph·∫£i lu√¥n JOIN v·ªõi b·∫£ng FB_LOCALITY ƒë·ªÉ l·∫•y FULL_NAME  like (v√† join ƒë·ªß 3 c·ªôt khi tr·∫£ ra t√™n).
- V·ªõi nh·ªØng c√¢u h·ªèi v·ªÅ s·ªë l∆∞·ª£ng bao nhi√™u B·∫ÆT BU·ªòC d√πng c√°c b·∫£ng PAKH_SLA_* v√¨ c√°c b·∫£ng n√†y ƒë√£ ch·ª©a c√°c s·ªë li·ªáu t·ªïng h·ª£p, KH√îNG C·∫¶N V√Ä KH√îNG ƒê∆Ø·ª¢C PH√âP JOIN v·ªõi b·∫£ng PAKH ho·∫∑c PAKH_CA_NHAN.
- Khi ng∆∞·ªùi d√πng ti·∫øp t·ª•c h·ªèi ‚Äúli·ªát k√™ c√°c ph·∫£n √°nh ƒë√≥‚Äù ‚Üí ph·∫£i chuy·ªÉn sang JOIN PAKH_CA_NHAN v√† PAKH qua ID ƒë·ªÉ l·∫•y ƒë·∫ßy ƒë·ªß th√¥ng tin t·ª´ b·∫£ng PAKH, v√† b·∫Øt bu·ªôc tr·∫£ ra danh s√°ch th√¥ng tin ph·∫£n √°nh t·ª´ b·∫£ng PAKH. 
- V·ªõi c√°c tr∆∞·ªùng m√£ nh∆∞ `LOAI_PHAN_ANH`, `FB_GROUP`, `HIEN_TUONG`, `NGUYEN_NHAN`, `DON_VI_NHAN` c·∫ßn JOIN b·∫£ng t∆∞∆°ng ·ª©ng (FB_TYPE, FB_GROUP, FB_HIEN_TUONG, FB_REASON, FB_DEPARTMENT) ƒë·ªÉ tr·∫£ ra t√™n (`NAME`) thay v√¨ tr·∫£ ra m√£.
- L∆ØU √ù: c·ªôt `TRANG_THAI` ch·ªâ t·ªìn t·∫°i trong c√°c b·∫£ng `PAKH_CA_NHAN`, `PAKH_PHONG_BAN`, `PAKH_TO_NHOM`, `PAKH_TRUNG_TAM` v·ªõi gi√° tr·ªã h·ª£p l·ªá l√†: 'TU_CHOI','HOAN_THANH','DONG','DANG_XU_LY','DA_XU_LY'.
{previous_error if previous_error else ''}
- N·∫øu c√≥ l·ªói tr∆∞·ªõc ƒë√≥, s·ª≠a l·ªói v√† t·∫°o l·∫°i c√¢u SQL ch√≠nh x√°c.
- N·∫øu ch·ªâ h·ªèi "bao nhi√™u" m√† kh√¥ng ƒë·ªÅ c·∫≠p "li·ªát k√™", **ch·ªâ c·∫ßn tr·∫£ v·ªÅ k·∫øt qu·∫£ t√≠nh**.
- N·∫øu c√¢u h·ªèi ti·∫øp n·ªëi, ∆∞u ti√™n context: {self.context["filters"]}

# RELATIONS
{self._format_relations_for_prompt()}

# Quan tr·ªçng
- TUY·ªÜT ƒê·ªêI KH√îNG B·ªäA T√äN B·∫¢NG, T√äN C·ªòT KH√îNG C√ì TRONG SCHEMA
- ∆Øu ti√™n d√πng ƒë√∫ng SELECT g·ª£i √Ω trong SCHEMA n·∫øu c√≥.
- N·∫øu kh√¥ng ƒë·ªß th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi, h√£y th√¥ng b√°o r√µ thay v√¨ ƒëo√°n.
- H·∫°n ch·∫ø JOIN kh√¥ng c·∫ßn thi·∫øt.

C√¢u h·ªèi:
{question_resolved}

SQL:
"""
                sql_raw = self._invoke_model(prompt_text)
                self.update_context(sql=sql_raw)
                return sql_raw

            except Exception as e:
                logger.error(f"L·ªói sinh SQL (l·∫ßn th·ª≠ {attempt + 1}): {e}")
                if attempt < retries - 1:
                    previous_error = str(e)
                    continue
                raise

    def _invoke_model(self, prompt_text: str, retries: int = 1) -> str:
        attempt = 0
        while attempt < retries:
            try:
                if self.engine == "openai":
                    from langchain.prompts import PromptTemplate
                    from langchain.chains import LLMChain

                    sql_prompt = PromptTemplate(input_variables=["input"], template="{input}")
                    sql_chain = LLMChain(llm=self.llm, prompt=sql_prompt)
                    return str(sql_chain.run(prompt_text)).strip()

                elif self.engine == "gemini":
                    return str(self.model.generate_content(prompt_text).text).strip()

            except Exception as e:
                error_msg = str(e).lower()
                if "rate limit" in error_msg:
                    attempt += 1
                    logger.warning(f"OpenAI rate-limit, retrying attempt {attempt}/{retries}...")
                    if attempt >= retries:
                        logger.warning("OpenAI v·∫´n rate-limit sau retry, fallback Gemini.")
                        self._fallback_to_gemini()
                        return self._invoke_model(prompt_text)
                else:
                    raise
    def _postprocess_sql(self, sql_raw: str, schema_text: str) -> str:
        sql = str(sql_raw or "").strip()
        if "fb_locality" in sql.lower():
            # √©p select th√™m province, district n·∫øu ch∆∞a c√≥
            if not re.search(r"\bprovince\b", sql, re.IGNORECASE):
                sql = sql.replace("SELECT", "SELECT l.PROVINCE, l.DISTRICT,", 1)
        return sql

    def _extract_year_from_question(self):
        if self.context.get("last_question"):
            match = re.search(r"\b(20[2-3][0-9])\b", str(self.context["last_question"]), re.IGNORECASE)
            if match:
                return match.group(1)
        return None
    import re


    def response(self, question: str, execute_fn=None, is_independent: bool = False, force_no_cache: bool = False):
        if is_independent:
            self.reset_context()

        if execute_fn is None:
            execute_fn = self.execute_sql

        # --- Ch·ªâ fuzzy match c√° nh√¢n n·∫øu c√¢u h·ªèi c√≥ t·ª´ kh√≥a li√™n quan ---
        dept_keywords_to_nhom = ["t·ªï", "nh√≥m"]
        dept_keywords_phong_ban = ["ph√≤ng ban", "ban"]

        dept_code, dept_name, score = None, None, 0

        # Ki·ªÉm tra t·ªï nh√≥m
        if contains_keyword(question, dept_keywords_to_nhom):
            dept_code, dept_name, score = fuzzy_match_department(question, kind="to_nhom")

        # Ki·ªÉm tra ph√≤ng ban
        elif contains_keyword(question, dept_keywords_phong_ban):
            dept_code, dept_name, score = fuzzy_match_department(question, kind="phong_ban")

        if dept_code:
            logger.info(f"üîé Fuzzy match: '{question}' ‚Üí {dept_name} ({dept_code}), score={score}")
            question += f" (M√£ ph√≤ng ban t∆∞∆°ng ·ª©ng: {dept_code})"

        canhan_code = None
        if contains_keyword(question, canhan_keywords):
            canhan_code = resolve_canhan(question)
            if canhan_code:
                question += f" (CA_NHAN: {canhan_code})"

        try:
            # --- Sinh SQL ---
            sql_raw = self.generate_sql(
                question, is_follow_up=not is_independent, force_no_cache=force_no_cache
            )
            sql_clean = re.sub(r"```sql|```", "", sql_raw, flags=re.IGNORECASE).strip()

            # --- Th·ª±c thi SQL ---
            try:
                result = execute_fn(sql_clean)
            except Exception as e:
                # N·∫øu SQL l·ªói ho√†n to√†n ‚Üí fallback
                prompt = f"C√¢u h·ªèi: {question}\nTr·∫£ l·ªùi ng·∫Øn g·ªçn, ƒë√∫ng ng√¥n ng·ªØ c√¢u h·ªèi:"
                return self._invoke_model(prompt)

            # --- X·ª≠ l√Ω k·∫øt qu·∫£ ---
            target = result.get("details") or (result if "rows" in result else None)
            if not target or not target.get("rows"):
                return "Xin ph√©p ƒë∆∞·ª£c th√¥ng tin t·ªõi Anh/Ch·ªã: hi·ªán t·∫°i kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ph√π h·ª£p."

            columns, rows_full = target["columns"], target["rows"]
            is_statistical, is_info_query, is_list_query = self.detect_query_type(question, target)

            formatted_result = {}
            filter_link = None

            if is_statistical:
                try:
                    # N·∫øu nhi·ªÅu d√≤ng ‚Üí convert th√†nh dict {col0: col1, ...}
                    if len(rows_full) > 1 and len(columns) >= 2:
                        formatted_result = {
                            "statistical": True,
                            "data": {row[0]: row[1] for row in rows_full}
                        }
                    else:
                        val = rows_full[0][0] if rows_full else 0
                        if isinstance(val, (int, float)):
                            formatted_result = {"total": val}
                        else:
                            try:
                                formatted_result = {"total": float(val)}
                            except Exception:
                                formatted_result = {"statistical": True, "value": str(val)}
                except Exception:
                    formatted_result = {"statistical": True, "message": "‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu th·ªëng k√™."}


            elif is_list_query:
                # l·∫•y preview v√†i d√≤ng + ƒë·∫øm t·ªïng
                row_preview = rows_full[:3]
                sql_upper = sql_clean.upper()
                has_handler = any(keyword.upper() in sql_upper for keyword in ['CA_NHAN', 'PHONG_BAN', 'TO_NHOM', 'TRUNG_TAM'])
                year_in_question = self._extract_year_from_question()

                # X√°c ƒë·ªãnh nƒÉm m·∫∑c ƒë·ªãnh
                if has_handler:
                    default_year = year_in_question or "2024"
                else:
                    default_year = "2024"
                    if "FB_DATE" in columns:
                        fb_date_value = rows_full[0][columns.index("FB_DATE")]
                        if fb_date_value:
                            default_year = str(fb_date_value)[:4]

                context_common = {"year": default_year}

                try:
                    if has_handler:
                        params = extract_form_detail_params_from_sql(
                            sql_clean, context_common, get_ma_dia_chi_fuzzy=get_ma_dia_chi_from_cache
                        )
                        base_url = "http://14.160.91.174:8180/smartw/feedback/form/detail.htm"
                    else:
                        params = extract_list_params_from_sql(
                            sql_clean, context_common, get_ma_dia_chi_fuzzy=get_ma_dia_chi_from_cache
                        )
                        base_url = "http://14.160.91.174:8180/smartw/feedback/list.htm"

                    filter_link = build_filter_url(base_url, params)
                    logger.info(f"[LINK] Link build th√†nh c√¥ng: {filter_link}")
                except Exception as e:
                    logger.error(f"[ERROR] X√¢y d·ª±ng link th·∫•t b·∫°i: {e}")
                    filter_link = None

                formatted_result = summarize_rows(columns, rows_full)
                formatted_result["count"] = len(rows_full)
                if filter_link:
                        formatted_result["link"] = f"[truy c·∫≠p t·∫°i ƒë√¢y]({filter_link})"


            else:
                # Tr∆∞·ªùng h·ª£p kh√°c ‚Üí show 1 record ƒë·∫ßu ti√™n
                formatted_result = {"columns": columns, "rows": rows_full[:1], "count": len(rows_full)}

            # --- Lu√¥n sinh ph·∫£n h·ªìi t·ª± nhi√™n ---
            prompt = f"""
    C√¢u h·ªèi: {question}
    K·∫øt qu·∫£ truy v·∫•n (JSON):
    {json.dumps(formatted_result, ensure_ascii=False)}

    Y√™u c·∫ßu:
    - Tr·∫£ l·ªùi c√πng NG√îN NG·ªÆ v·ªõi c√¢u h·ªèi.
    - Tr·∫£ l·ªùi l·ªãch s·ª±, c√≥ x∆∞ng h√¥ ph√π h·ª£p.
    - KHI ƒê√É C√ì D·ªÆ LI·ªÜU TRUY V·∫§N ƒê∆Ø·ª¢C TH√å PH·∫¢I TR·∫¢ L·ªúI B√ÅM S√ÅT V√ÄO D·ªÆ LI·ªÜU, TUY·ªÜT ƒê·ªêI KH√îNG T·ª∞ B·ªäA ƒê·∫∂T TH√äM
    - N·∫øu c√≥ 'count', h√£y n√™u r√µ s·ªë l∆∞·ª£ng b·∫£n ghi.
    - N·∫øu c√≥ 'link', cu·ªëi c√¢u th√™m: "üîó {formatted_result.get('link')}".
    """
            answer = self._invoke_model(prompt)
            self.update_context(sql=sql_clean, result=result, question=question)
            return answer

        except Exception as e:
            # fallback cu·ªëi c√πng
            prompt = f"""
    Ng∆∞·ªùi d√πng h·ªèi: {question}

    Y√™u c·∫ßu:
    - Tr·∫£ l·ªùi ƒë√∫ng ng√¥n ng·ªØ c√¢u h·ªèi.
    - Tr·∫£ l·ªùi l·ªãch s·ª±, c√≥ x∆∞ng h√¥ ph√π h·ª£p.
    - B√ÅM S√ÅT d·ªØ li·ªáu (n·∫øu c√≥).
    """
            return self._invoke_model(prompt)



    def detect_query_type(self, question: str, result: dict):
        q = str(question or "").lower()
        rows = result.get("rows") or result.get("details", {}).get("rows")

        # N·∫øu k·∫øt qu·∫£ ch·ªâ c√≥ 1 c·ªôt v√† t√™n c·ªôt ch·ª©a "COUNT" ‚Üí ch·∫Øc ch·∫Øn l√† th·ªëng k√™
        if rows and len(rows[0]) == 1 and "count" in (result.get("columns", [""])[0].lower()):
            return True, False, False  # statistical

        is_list_query = any(kw in q for kw in [
            "li·ªát k√™", "danh s√°ch", "list", "show", "c√°c pa",
            "c√°c ph·∫£n √°nh", "xem chi ti·∫øt", "chi ti·∫øt c√°c ph·∫£n √°nh", "pa ·ªü","lk"
        ])
        is_statistical = any(kw in q for kw in [
            "theo t·ª´ng nh√≥m", "theo t·ª´ng lo·∫°i", "th·ªëng k√™",
            "t·ª´ng nh√≥m", "t·ª´ng lo·∫°i", "m·ªói nh√≥m", "m·ªói lo·∫°i",
            "bao nhi√™u", "t·ªïng s·ªë", "t·ªïng c·ªông", "s·ªë l∆∞·ª£ng",
            "bao nhi√™u ph·∫£n √°nh", "bn ph·∫£n √°nh", "bn pa", "bao nhi√™u pa"
        ]) or "group by" in q

        is_info_query = not is_list_query and not is_statistical and bool(rows)
        return is_statistical, is_info_query, is_list_query


    def clear_memory(self):
        if self.engine == "openai":
            self.memory.clear()
        else:
            self.memory = []
        logger.info("ƒê√£ x√≥a b·ªô nh·ªõ h·ªôi tho·∫°i.")

    def clear_cache(self):
        self.sql_cache.clear()
        self.schema_cache.clear()
        logger.info("ƒê√£ x√≥a SQL cache v√† schema cache.")

    def clear_all(self):
        self.clear_cache()
        self.clear_memory()
        self.last_context = {}
        logger.info("ƒê√£ x√≥a to√†n b·ªô b·ªô nh·ªõ v√† cache.")