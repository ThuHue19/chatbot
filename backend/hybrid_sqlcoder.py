import os
import re
import logging
from dotenv import load_dotenv
from utils.sql_planner import SQLPlanner
from utils.column_mapper import extract_tables_and_columns, generate_column_mapping_hint
from utils.schema_loader import TABLE_KEYWORDS, extract_relevant_tables
from utils.relation_loader import load_relations
from typing import List, Dict
import json
import unidecode
from db_utils import get_connection

conn = get_connection()


# --- Setup m√¥i tr∆∞·ªùng & logger ---
load_dotenv()
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

LLM_ENGINE = os.getenv("LLM_ENGINE", "openai").lower()
FORM_DETAIL_PARAMS = [
    "type", "trungTam", "phongBan", "toNhom", "caNhan", "loaiPa", "loaiThueBao",
    "nguyenNhan", "caTruc", "fo_bo", "tinh", "ctdv", "day", "today", "week",
    "month", "quarter", "year", "kpiName", "quanHuyen", "level", "nhomNguyenNhan"
]
LIST_PARAMS = [
    "tab", "id", "year", "tinhThanhPho", "quanHuyen", "is_ticket", "phong", "to", "caTruc",
    "dept_xuly", "dept", "ttTicket", "nhomPa", "loaiPa", "fo_bo", "loaiThueBao", "is_dongpa", "ctdv"
]
def normalize(text):
    return unidecode.unidecode(text).replace(' ', '_').replace('-', '_').lower()

def extract_form_detail_params_from_sql(sql: str, context: dict = None) -> Dict[str, str]:
    default_values = {k: "" for k in FORM_DETAIL_PARAMS}
    default_values.update({
        "type": "year",
        "today": "undefined",
        "week": "undefined",
        "month": "undefined",
        "quarter": "undefined",
    })
    params = default_values.copy()
    regex_map = {
        "trungTam": r"TRUNG_TAM\s*=\s*'([^']+)'",
        "phongBan": r"PHONG_BAN\s*=\s*'([^']+)'",
        "toNhom": r"TO_NHOM\s*=\s*'([^']+)'",
        "caNhan": r"CA_NHAN\s*=\s*'([^']+)'",
        "loaiPa": r"LOAI_PA\s*=\s*'([^']+)'",
        "loaiThueBao": r"LOAI_THUE_BAO\s*=\s*'([^']+)'",
        "nguyenNhan": r"NGUYEN_NHAN\s*=\s*'([^']+)'",
        "caTruc": r"CA_TRUC\s*=\s*'([^']+)'",
        "fo_bo": r"FO_BO\s*=\s*'([^']+)'",
        "tinh": r"TINH\s*=\s*'([^']+)'",
        "ctdv": r"CTDV\s*=\s*'([^']+)'",
        "day": r"DAY\s*=\s*'([^']+)'",
        "week": r"WEEK\s*=\s*'([^']+)'",
        "month": r"MONTH\s*=\s*'([^']+)'",
        "quarter": r"QUARTER\s*=\s*'([^']+)'",
        "year": r"YEAR\s*=\s*'([^']+)'",
        "kpiName": r"KPI_NAME\s*=\s*'([^']+)'",
        "quanHuyen": r"QUAN_HUYEN\s*=\s*'([^']+)'",
        "level": r"LEVEL\s*=\s*'([^']+)'",
        "nhomNguyenNhan": r"NHOM_NGUYEN_NHAN\s*=\s*'([^']+)'",
    }
    for key, regex in regex_map.items():
        match = re.search(regex, sql, re.IGNORECASE)
        if match:
            params[key] = match.group(1)
    if context:
        for key in params:
            if key in context and context[key]:
                params[key] = context[key]
    if not params["year"]:
        match = re.search(r"\b(20[2-3][0-9])\b", sql)
        if match:
            params["year"] = match.group(1)
        elif context and "year" in context and context["year"]:
            params["year"] = context["year"]
    # G·ª£i √Ω t·ª± ƒë·ªông cho type, level, kpiName n·∫øu c√≥ t·ª´ kh√≥a ƒë·∫∑c bi·ªát trong SQL
    if not params["kpiName"]:
        trang_thai_map = {
            "TU_CHOI": "soPaTuChoi",
            "DONG": "soPaDong",
            "DANG_XU_LY": "soPaDangXlDh",
            "DA_XU_LY": "soPaDaXlyDh",
            "HOAN_THANH": "soPaHoanThanh",
        }
        match = re.search(r"TRANG_THAI\s*=\s*'([^']+)'", sql, re.IGNORECASE)
        if match:
            trang_thai = match.group(1).upper()
            if trang_thai in trang_thai_map:
                params["kpiName"] = trang_thai_map[trang_thai]
    if not params["level"]:
        if "PAKH_CA_NHAN" in sql.upper():
            params["level"] = "ca_nhan"
        elif "PAKH_TO_NHOM" in sql.upper():
            params["level"] = "to_nhom"
        elif "PAKH_PHONG_BAN" in sql.upper():
            params["level"] = "phong_ban"
        elif "PAKH_TRUNG_TAM" in sql.upper():
            params["level"] = "trung_tam"
    return params

def extract_list_params_from_sql(sql: str, context: dict = None) -> Dict[str, str]:
    default_values = {k: "" for k in LIST_PARAMS}
    default_values.update({
        "tab": "year",
        "is_ticket": "Y%2CN",
        "is_dongpa": "DANG_XU_LY%2CDA_XU_LY",
    })
    params = default_values.copy()
    regex_map = {
        "tab": r"tab\s*=\s*'([^']+)'",
        "id": r"id\s*=\s*'([^']+)'",
        "year": r"year\s*=\s*'([^']+)'",
        "tinhThanhPho": r"TINH_THANH_PHO\s*=\s*'([^']+)'",
        "quanHuyen": r"QUAN_HUYEN\s*=\s*'([^']+)'",
        "is_ticket": r"is_ticket\s*=\s*'([^']+)'",
        "phong": r"PHONG\s*=\s*'([^']+)'",
        "to": r"TO_NHOM\s*=\s*'([^']+)'",
        "caTruc": r"CA_TRUC\s*=\s*'([^']+)'",
        "dept_xuly": r"DEPT_XULY\s*=\s*'([^']+)'",
        "dept": r"DEPT\s*=\s*'([^']+)'",
        "ttTicket": r"TT_TICKET\s*=\s*'([^']+)'",
        "nhomPa": r"NHOM_PA\s*=\s*'([^']+)'",
        "loaiPa": r"LOAI_PA\s*=\s*'([^']+)'",
        "fo_bo": r"FO_BO\s*=\s*'([^']+)'",
        "loaiThueBao": r"LOAI_THUE_BAO\s*=\s*'([^']+)'",
        "is_dongpa": r"is_dongpa\s*=\s*'([^']+)'",
        "ctdv": r"CTDV\s*=\s*'([^']+)'"
    }
    for key, regex in regex_map.items():
        match = re.search(regex, sql, re.IGNORECASE)
        if match:
            params[key] = match.group(1)
    if context:
        for key in params:
            # LU√îN ∆ØU TI√äN context n·∫øu context c√≥ gi√° tr·ªã
            if key in context and context[key]:
                params[key] = context[key]
    return params

def build_filter_url(base: str, params: Dict[str, str]) -> str:
    query = "&".join(f"{k}={v}" for k, v in params.items())
    return f"{base}?{query}"

# --- L·ªõp Cache cho Schema ---
class SchemaCache:
    def __init__(self):
        self.cache = {}

    def get_schema(self, table_name):
        if table_name not in self.cache:
            from utils.schema_loader import load_schema
            self.cache[table_name] = load_schema(table_name)
        return self.cache[table_name]

    def clear(self):
        self.cache.clear()
        logger.info("ƒê√£ x√≥a schema cache.")

# --- L·ªõp ch√≠nh HybridSQLCoder ---
class HybridSQLCoder:
    def __init__(self, db_conn=None):
        self.engine = LLM_ENGINE
        self.sql_cache = {}
        self.schema_cache = SchemaCache()
        self.relations, self.concat_mappings = load_relations()
        self.last_question = None
        self.last_sql = None
        self.last_result = None
        self.query_mode = None
        self.last_user_subject = None
        self.last_user_subjects = set()
        self.last_context = {}  # T·ªïng h·ª£p m·ªçi filter context
        self.db_conn = db_conn
        BASE_DIR = os.path.dirname(os.path.abspath(__file__))
        EXAMPLES_FILE = os.path.join(BASE_DIR, "utils", "sql_examples.json")
        self.sql_examples = self._load_sql_examples(EXAMPLES_FILE)

        if self.engine == "openai":
            self._init_openai()
        else:
            self._init_gemini()

    def _init_openai(self):
        from langchain_openai import ChatOpenAI
        from langchain.memory import ConversationSummaryMemory

        self.model_name = os.getenv("OPENAI_MODEL", "gpt-4")
        self.llm = ChatOpenAI(model=self.model_name, temperature=0)
        self.memory = ConversationSummaryMemory(
            llm=self.llm, memory_key="chat_history", return_messages=True
        )

    def _init_gemini(self):
        import google.generativeai as genai

        api_key = os.getenv("GEMINI_API_KEY")
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel("models/gemini-2.5-flash", generation_config={"temperature": 0})
        self.memory = []  # list of (user_message, ai_message)

    def _fallback_to_gemini(self):
        logger.warning("OpenAI g·∫∑p l·ªói, chuy·ªÉn sang Gemini.")
        self._init_gemini()
        self.engine = "gemini"

    # --- Helper ---
    def _get_last_n_user_questions(self, n=5):
        if self.engine == "openai" and hasattr(self.memory, "chat_memory"):
            return [
                msg.content for msg in reversed(self.memory.chat_memory.messages)
                if msg.type == "human"
            ][:n][::-1]
        elif self.engine == "gemini":
            return [user for user, _ in self.memory[-n:]][::-1]
        return []

    def _update_context_from_sql(self, sql: str):
        # D√πng regex ƒë·ªÉ b√≥c c√°c filter ph·ªï bi·∫øn
        context = {}
        patterns = {
            "phongBan": r"PHONG_BAN\s*=\s*'([^']+)'",
            "toNhom": r"TO_NHOM\s*=\s*'([^']+)'",
            "trungTam": r"TRUNG_TAM\s*=\s*'([^']+)'",
            "caNhan": r"CA_NHAN\s*=\s*'([^']+)'",
            "tinhThanhPho": r"TINH_THANH_PHO\s*=\s*'([^']+)'",
            "quanHuyen": r"QUAN_HUYEN\s*=\s*'([^']+)'",
            "nhomPa": r"NHOM_PA\s*=\s*'([^']+)'",
            "loaiPa": r"LOAI_PA\s*=\s*'([^']+)'",
            "maTinh": r"MA_TINH\s*=\s*'([^']+)'",
            "maHuyen": r"MA_HUYEN\s*=\s*'([^']+)'",
            "level": r"LEVEL\s*=\s*'([^']+)'",
            "nhomNguyenNhan": r"NHOM_NGUYEN_NHAN\s*=\s*'([^']+)'",
        }
        for key, pat in patterns.items():
            m = re.search(pat, sql, re.IGNORECASE)
            if m:
                context[key] = m.group(1)
        if context:
            self.last_context.update(context)
    # File: hybrid_sqlcoder.py

    def get_ma_dia_chi_fuzzy(self, dia_chi_text: str) -> dict:
        if not self.db_conn:
            logger.warning("[WARN] DB connection is None. Kh√¥ng th·ªÉ truy v·∫•n m√£ ƒë·ªãa l√Ω.")
            return {}
        
        # Chu·∫©n h√≥a ƒë·ªãa ch·ªâ: chuy·ªÉn lowercase, lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát
        text = unidecode.unidecode(dia_chi_text).lower()
        text = re.sub(r"[^\w\s]", " ", text)
        keywords = [kw.strip() for kw in text.split() if kw.strip()]
        if not keywords:
            return {}

        # T·∫°o ƒëi·ªÅu ki·ªán WHERE ƒë·ªông
        conditions = []
        params = []
        for i, kw in enumerate(keywords):
            # Thay th·∫ø unaccent(LOWER(full_name)) b·∫±ng LOWER(full_name) ƒë·ªÉ tr√°nh l·ªói ORA-00904
            conditions.append(f"LOWER(full_name) LIKE :{i+1}")
            params.append(f"%{kw}%")
        
        where_clause = " AND ".join(conditions)
        query = f"""
            SELECT ma_tinh AS tinhThanhPho, ma_huyen AS quanHuyen 
            FROM FB_LOCALITY 
            WHERE {where_clause}
            ORDER BY LENGTH(full_name) DESC
            FETCH FIRST 1 ROWS ONLY
        """

        try:
            with self.db_conn.cursor() as cursor:
                cursor.execute(query, params)
                row = cursor.fetchone()
                if row:
                    result = {
                        "tinhThanhPho": row[0],
                        "quanHuyen": row[1]
                    }
                    logger.info(f"[DEBUG] Mapping ƒë·ªãa ch·ªâ '{dia_chi_text}' -> {result}")
                    return result
        except Exception as e:
            logger.error(f"L·ªói truy v·∫•n m√£ ƒë·ªãa l√Ω: {e}")
        return {}

    def _extract_dia_chi_from_question(self, question: str) -> str:
        # M·ªü r·ªông pattern b·∫Øt ƒë·ªãa ch·ªâ
        patterns = [
            r"(?:t·∫°i|·ªü|ƒë·ªãa ch·ªâ|ƒë·ªãa b√†n|t·∫°i khu v·ª±c|t·∫°i)\s*([^,.;?]+)",
            r"(\b(?:qu·∫≠n|huy·ªán|th·ªã x√£|tp\.?|t·ªânh)\s+[\w\s]+)",
            r"([\w\s]+(?:,\s*[\w\s]+){1,2})"
        ]
        
        for pattern in patterns:
            match = re.search(pattern, question, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        # Fallback: l·∫•y c·ª•m cu·ªëi c√πng n·∫øu kh√¥ng kh·ªõp pattern
        return " ".join(question.split()[-3:])
    def _detect_new_filter(self, question: str) -> bool:
        # N·∫øu c√¢u h·ªèi c√≥ t·ª´ kh√≥a v·ªÅ ph√≤ng ban, t·ªï nh√≥m, trung t√¢m, ƒë·ªãa b√†n, nh√≥m, lo·∫°i...
        keywords = [
            "ph√≤ng", "ph√≤ng ban", "t·ªï", "t·ªï nh√≥m", "trung t√¢m", "ƒë·ªãa b√†n", "t·ªânh", "huy·ªán",
            "nh√≥m ph·∫£n √°nh", "lo·∫°i ph·∫£n √°nh", "c√° nh√¢n", "s·ªë thu√™ bao", "m√£ t·ªânh", "m√£ huy·ªán"
        ]
        q = question.lower()
        return any(kw in q for kw in keywords)

    def _resolve_follow_up_question(self, question: str, is_follow_up: bool = False) -> str:
        """"D√πng LLM ƒë·ªÉ di·ªÖn ƒë·∫°t l·∫°i c√¢u h·ªèi ti·∫øp n·ªëi th√†nh c√¢u ƒë·∫ßy ƒë·ªß, c√≥ th·ªÉ hi·ªÉu ƒë·ªôc l·∫≠p"""
        if not is_follow_up:
            return question
        last_questions = self._get_last_n_user_questions(n=1)
        prompt  = f"""
B·∫°n l√† tr·ª£ l√Ω ng√¥n ng·ªØ.
D∆∞·ªõi ƒë√¢y l√†  c√¢u h·ªèi tr∆∞·ªõc c·ªßa ng∆∞·ªùi d√πng:
{os.linesep.join(f"{i+1}. {q}" for i, q in enumerate(last_questions))}

C√¢u h·ªèi hi·ªán t·∫°i:
{question}

H√£y vi·∫øt l·∫°i c√¢u h·ªèi hi·ªán t·∫°i th√†nh m·ªôt c√¢u h·ªèi ƒë·∫ßy ƒë·ªß, r√µ nghƒ©a, c√≥ th·ªÉ hi·ªÉu ƒë·ªôc l·∫≠p. 
N·∫øu c√≥ t·ª´ nh∆∞ ‚Äúƒë√≥‚Äù, ‚Äún√†y‚Äù, ‚Äúng∆∞·ªùi ƒë√≥‚Äù,... th√¨ h√£y thay th·∫ø b·∫±ng th√¥ng tin ng·ªØ c·∫£nh t·ª´ c√°c c√¢u h·ªèi tr∆∞·ªõc.
Ch·ªâ tr·∫£ l·∫°i c√¢u h·ªèi m·ªõi, kh√¥ng th√™m gi·∫£i th√≠ch.
"""
        resolved = self._invoke_model(prompt)
        return resolved.strip() if resolved else question

    def _format_memory_context(self):
        history = []
        if self.engine == "openai" and hasattr(self.memory, "chat_memory"):
            for m in self.memory.chat_memory.messages[-10:]:
                prefix = "Ng∆∞·ªùi d√πng" if m.type == "human" else "Tr·ª£ l√Ω"
                history.append(f"{prefix}: {m.content}")
        elif self.engine == "gemini":
            for user_msg, ai_msg in self.memory[-5:]:
                history.extend([f"Ng∆∞·ªùi d√πng: {user_msg}", f"Tr·ª£ l√Ω: {ai_msg}"])
        return os.linesep.join(history)

    def _is_statistical_list(self, question: str) -> bool:
        q = question.lower()
        return any(kw in q for kw in [
            "theo t·ª´ng nh√≥m", "theo t·ª´ng lo·∫°i", "th·ªëng k√™", "group by", "t·ª´ng nh√≥m", "t·ª´ng lo·∫°i", "m·ªói nh√≥m", "m·ªói lo·∫°i"
        ])

    def _format_relations_for_prompt(self):
        return os.linesep.join(
            f"- {table}.{col} ‚Üí {ref['ref_table']}.{ref['ref_column']}"
            for table, cols in self.relations.items()
            for col, ref in cols.items()
        )

    def _generate_sum_hint(self) -> str:
        hints = [
            "- ƒê·ªëi v·ªõi c√°c b·∫£ng SLA (PAKH_SLA_*):",
            "   - Lu√¥n d√πng SUM() v·ªõi c√°c c·ªôt t·ªïng h·ª£p nh∆∞ SO_PA_NHAN, SO_PA_DA_XL, SO_PA_QH.",
            "   - √Ånh x·∫° t·ª´ kh√≥a trong c√¢u h·ªèi sang c·ªôt t·ªïng h·ª£p:",
            "     - 'qu√° h·∫°n' ‚Üí SUM(SO_PA_QH) (S·ªë ph·∫£n √°nh qu√° h·∫°n)",
            "     - 'ƒë√£ x·ª≠ l√Ω ƒë√∫ng h·∫°n' ‚Üí SUM(SO_PA_DA_XL_DH)",
            "     - 'ƒëang x·ª≠ l√Ω ƒë√∫ng h·∫°n' ‚Üí SUM(SO_PA_DANG_XL_DH)",
            "     - 't·ª´ ch·ªëi' ho·∫∑c 'b·ªã t·ª´ ch·ªëi' ‚Üí SUM(SO_PA_TU_CHOI)",
            "     - 't·ªïng th·ªùi gian x·ª≠ l√Ω' ‚Üí SUM(TONG_TG_XL) / 60 (ƒë·ªïi sang gi·ªù)",
            "     - 'th·ªùi gian trung b√¨nh x·ª≠ l√Ω' ‚Üí  ROUND(SUM(TONG_TG_XL) / NULLIF(SUM(SO_PA_DA_XL), 0), 2) AS TB_PHUT, TRUNC(SUM(TONG_TG_XL) / NULLIF(SUM(SO_PA_DA_XL), 0) / 60) AS GIO, MOD(ROUND(SUM(TONG_TG_XL) / NULLIF(SUM(SO_PA_DA_XL), 0)), 60) AS PHUT",
            "   - TUY·ªÜT ƒê·ªêI KH√îNG JOIN b·∫£ng SLA tr·ª±c ti·∫øp v·ªõi PAKH ho·∫∑c PAKH_CA_NHAN cho c√°c truy v·∫•n th·ªëng k√™.",
            "- C√¥ng th·ª©c t√≠nh t·ª∑ l·ªá x·ª≠ l√Ω ƒë√∫ng h·∫°n: (SUM(SO_PA_DA_XL_DH) + SUM(SO_PA_DANG_XL_DH)) / NULLIF(SUM(SO_PA_NHAN), 0) * 100.",
            "- D√πng b·∫£ng SLA ph√π h·ª£p v·ªõi ng·ªØ c·∫£nh c√° nh√¢n, t·ªï nh√≥m, ph√≤ng ban, trung t√¢m",
            "- L·∫•y t√™n c√° nh√¢n trong c·ªôt CA_NHAN"
        ]
        return os.linesep.join(hints)

    def _extract_last_user_subject_from_result(self, result: dict):
        target_result = result.get("details") or result
        if not target_result or "columns" not in target_result or "rows" not in target_result:
            return
        try:
            for colname in ["CA_NHAN", "SO_THUE_BAO", "ID"]:
                if colname in target_result["columns"]:
                    idx = target_result["columns"].index(colname)
                    for row in target_result["rows"]:
                        value = str(row[idx])
                        if value:
                            self.last_user_subject = value
                            self.last_user_subjects.add(value)
                            logger.info(f"‚úÖ C·∫≠p nh·∫≠t last_user_subject: {value}")
        except Exception as e:
            logger.warning(f"L·ªói khi tr√≠ch last_user_subject t·ª´ k·∫øt qu·∫£: {e}")

    def _load_sql_examples(self, file_path: str) -> List[Dict[str, str]]:
        if os.path.exists(file_path):
            with open(file_path, "r", encoding="utf-8") as f:
                return json.load(f)
        logger.warning(f"File v√≠ d·ª• SQL kh√¥ng t√¨m th·∫•y: {file_path}")
        return []

    def _select_relevant_examples(self, question: str, num_examples: int = 3) -> str:
        question_lower = question.lower()
        question_words = set(question_lower.split())
        scored_examples = []

        for example in self.sql_examples:
            example_question_lower = example["question"].lower()
            example_sql_upper = example["sql"].upper()

            score = 0

            common_keywords = question_words.intersection(set(example_question_lower.split()))
            score += len(common_keywords) * 2

            if ("m·ªói" in question_lower or "t·ª´ng" in question_lower) and \
               ("m·ªói" in example_question_lower or "t·ª´ng" in example_question_lower):
                score += 5

            for table, info in TABLE_KEYWORDS.items():
                if any(kw.lower() in question_lower for kw in info.get("keywords", [])) and \
                   table.upper() in example_sql_upper:
                    score += 4

            if "m·ªói t·ªï" in question_lower and \
               "TO_NHOM" in example_sql_upper and \
               "GROUP BY" in example_sql_upper and \
               "PAKH_SLA_TO_NHOM" in example_sql_upper and \
               "JOIN FB_GROUP" not in example_sql_upper:
                score += 20

            if score > 0:
                scored_examples.append((score, example))

        scored_examples.sort(key=lambda x: x[0], reverse=True)

        relevant_examples = []
        added_questions = set()

        for score, example in scored_examples:
            if example["question"] not in added_questions:
                relevant_examples.append(example)
                added_questions.add(example["question"])
            if len(relevant_examples) >= num_examples:
                break

        if len(relevant_examples) < num_examples:
            for ex in self.sql_examples:
                if ex["question"] not in added_questions:
                    relevant_examples.append(ex)
                    added_questions.add(ex["question"])
                if len(relevant_examples) >= num_examples:
                    break

        example_str = ""
        if relevant_examples:
            example_str += os.linesep + "---EXAMPLES START---" + os.linesep
            for ex in relevant_examples:
                example_str += f"C√¢u h·ªèi: {ex['question']}" + os.linesep + "SQL:" + os.linesep + f"```sql{os.linesep}{ex['sql']}{os.linesep}```" + os.linesep + os.linesep
            example_str += "---EXAMPLES END---" + os.linesep + os.linesep
        return example_str

    # --- Sinh SQL ---
    def generate_sql(self, question: str, is_follow_up: bool = False, previous_error: str = None, retries: int = 2, force_no_cache: bool = False):
        key = question.strip().lower()
        self.last_question = question
        # N·∫øu c√≥ ng·ªØ c·∫£nh ng∆∞·ªùi d√πng, th√™m v√†o cache key ƒë·ªÉ ph√¢n bi·ªát
        if self.last_user_subjects:
            key += "__" + "__".join(sorted(sub.lower() for sub in self.last_user_subjects))

        # S·ª≠ d·ª•ng cache n·∫øu c√≥ v√† kh√¥ng force_no_cache
        if not force_no_cache and key in self.sql_cache:
            logger.info(f"[CACHE] S·ª≠ d·ª•ng SQL ƒë√£ cache cho c√¢u h·ªèi: {question}")
            return self.sql_cache[key]

        for attempt in range(retries):
            try:
                # N·∫øu l√† c√¢u h·ªèi ti·∫øp n·ªëi, b·ªï sung context filter v√†o prompt n·∫øu kh√¥ng c√≥ filter m·ªõi
                context_for_prompt = ""
                if is_follow_up and self.last_context and not self._detect_new_filter(question):
                    context_for_prompt = (
                        "\n# Ng·ªØ c·∫£nh h·ªôi tho·∫°i tr∆∞·ªõc ƒë√≥:\n"
                        + "\n".join(f"- {k}: {v}" for k, v in self.last_context.items())
                        + "\n"
                    )

                current_question_for_resolution = question
                if is_follow_up and self.last_user_subjects:
                    joined_subjects = ", ".join(self.last_user_subjects)
                    current_question_for_resolution = f"{question.strip()} (li√™n quan ƒë·∫øn c√°c c√° nh√¢n: {joined_subjects})"
                    logger.info(f"ƒê√£ th√™m ng·ªØ c·∫£nh c√° nh√¢n v√†o c√¢u h·ªèi ti·∫øp n·ªëi: {current_question_for_resolution}")

                question_resolved = self._resolve_follow_up_question(current_question_for_resolution, is_follow_up)

                planner = SQLPlanner(self._invoke_model)
                plan_result = planner.plan(question_resolved)
                relevant_tables = plan_result.get("tables", [])
                relevant_examples_str = self._select_relevant_examples(question)

                if not relevant_tables:
                    logger.warning("Planner kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c b·∫£ng ‚Üí fallback extract_relevant_tables")
                    relevant_tables = extract_relevant_tables(question_resolved)

                if any(kw in question_resolved.lower() for kw in ["li·ªát k√™", "c√°c ph·∫£n √°nh"]):
                    if "PAKH_CA_NHAN" not in relevant_tables:
                        relevant_tables.append("PAKH_CA_NHAN")
                    if "PAKH" not in relevant_tables and "PAKH_NOI_DUNG_PHAN_ANH" not in relevant_tables:
                        relevant_tables.append("PAKH")

                if "FB_LOCALITY" not in relevant_tables:
                    relevant_tables.append("FB_LOCALITY")

                schema_text = os.linesep + os.linesep.join(
                    self.schema_cache.get_schema(tbl) for tbl in relevant_tables if tbl
                )
                if not schema_text:
                    raise ValueError("Kh√¥ng t√¨m th·∫•y schema ph√π h·ª£p.")

                column_mapping_hint = generate_column_mapping_hint(question_resolved)

                prompt_text = f"""
{context_for_prompt}
B·∫°n l√† tr·ª£ l√Ω sinh truy v·∫•n SQL cho CSDL Oracle.
# M·ª•c ti√™u
Sinh truy v·∫•n SQL ch√≠nh x√°c. ∆Øu ti√™n tr·∫£ l·ªùi nhanh ch√≥ng, ƒë∆°n gi·∫£n.

# L·ªãch s·ª≠
{self._format_memory_context()}

{relevant_examples_str}

# √Ånh x·∫° t·ª´ t·ª´ kh√≥a ‚Üí b·∫£ng.c·ªôt
{column_mapping_hint}
        
# SCHEMA
{schema_text}

# H∆∞·ªõng d·∫´n
{self._generate_sum_hint()}
- Ch·∫•p nh·∫≠n c√°c h√¨nh th·ª©c vi·∫øt t·∫Øt, v√≠ d·ª• p/a t∆∞∆°ng ·ª©ng v·ªõi ph·∫£n √°nh.
- V·ªõi s·ªë thu√™ bao ng∆∞·ªùi d√πng, trong c∆° s·ªü d·ªØ li·ªáu ƒëang c√≥ ƒë·ªãnh d·∫°ng kh√¥ng c√≥ s·ªë 0 ·ªü ƒë·∫ßu, khi ng∆∞·ªùi d√πng nh·∫≠p c√¢u h·ªèi B·∫ÆT BU·ªòC ch·∫•p nh·∫≠n c·∫£ ki·ªÉu nh·∫≠p C√ì S·ªê 0 v√† KH√îNG c√≥ s·ªë 0.
- v·ªõi nh·ªØng c√¢u h·ªèi li·ªát k√™ ph·∫£n √°nh, th√¥ng tin ph·∫£n √°nh B·∫ÆT BU·ªòC SELECT SO_THUE_BAO, NOI_DUNG_PHAN_ANH ƒë·ªÉ ch·ªâ c·∫ßn l·∫•y th√¥ng tin t·ª´ c·ªôt n√†y trong b·∫£ng PAKH
- ∆ØU TI√äN xem m·ª©c ƒë·ªô t∆∞∆°ng th√≠ch v·ªõi t·ª´ kh√≥a trong table_keywords.json ƒë·ªÉ truy v·∫•n v√† tr·∫£ ra ƒë√∫ng c·ªôt ƒë∆∞·ª£c h·ªèi, kh√¥ng tr·∫£ l·ªùi thi·∫øu hay th·ª´a th√¥ng tin, n·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ n·ªôi dung SIM, g√≥i c∆∞·ªõc, m·∫°ng y·∫øu,... th√¨ th√¥ng tin s·∫Ω ƒë∆∞·ª£c l∆∞u trong b·∫£ng `PAKH_NOI_DUNG_PHAN_ANH`, KH√îNG PH·∫¢I b·∫£ng `PAKH`.
- V·ªõi nh·ªØng c√¢u h·ªèi v·ªÅ s·ªë l∆∞·ª£ng bao nhi√™u B·∫ÆT BU·ªòC d√πng c√°c b·∫£ng PAKH_SLA_* v√¨ c√°c b·∫£ng n√†y ƒë√£ ch·ª©a c√°c s·ªë li·ªáu t·ªïng h·ª£p, KH√îNG C·∫¶N V√Ä KH√îNG ƒê∆Ø·ª¢C PH√âP JOIN v·ªõi b·∫£ng PAKH ho·∫∑c PAKH_CA_NHAN.
- Khi ng∆∞·ªùi d√πng ti·∫øp t·ª•c h·ªèi ‚Äúli·ªát k√™ c√°c ph·∫£n √°nh ƒë√≥‚Äù ‚Üí ph·∫£i chuy·ªÉn sang JOIN PAKH_CA_NHAN v√† PAKH qua ID ƒë·ªÉ l·∫•y ƒë·∫ßy ƒë·ªß th√¥ng tin t·ª´ b·∫£ng PAKH, v√† b·∫Øt bu·ªôc tr·∫£ ra danh s√°ch th√¥ng tin ph·∫£n √°nh t·ª´ b·∫£ng PAKH. 
- N·∫øu c√¢u h·ªèi ti·∫øp n·ªëi ch·ªâ l√† "li·ªát k√™" ho·∫∑c "li·ªát k√™ ƒëi", h√£y li·ªát k√™ t·∫•t c·∫£ c√°c ph·∫£n √°nh c·ªßa c√° nh√¢n ƒë√£ h·ªèi tr∆∞·ªõc ƒë√≥ KH√îNG t·ª± ƒë·ªông th√™m ƒëi·ªÅu ki·ªán tr·∫°ng th√°i b·ªã t·ª´ ch·ªëi/tr·∫°ng th√°i kh√°c n·∫øu c√¢u h·ªèi kh√¥ng n√™u r√µ.
- V·ªõi c√°c tr∆∞·ªùng m√£ nh∆∞ `LOAI_PHAN_ANH`, `FB_GROUP`, `HIEN_TUONG`, `NGUYEN_NHAN`, `DON_VI_NHAN` c·∫ßn JOIN b·∫£ng t∆∞∆°ng ·ª©ng (FB_TYPE, FB_GROUP, FB_HIEN_TUONG, FB_REASON, FB_DEPARTMENT) ƒë·ªÉ tr·∫£ ra t√™n (`NAME`) thay v√¨ tr·∫£ ra m√£.
- L∆ØU √ù: c·ªôt `TRANG_THAI` ch·ªâ t·ªìn t·∫°i trong c√°c b·∫£ng `PAKH_CA_NHAN`, `PAKH_PHONG_BAN`, `PAKH_TO_NHOM`, `PAKH_TRUNG_TAM` v·ªõi gi√° tr·ªã h·ª£p l·ªá l√† c√°c m√£ vi·∫øt hoa kh√¥ng d·∫•u: 'TU_CHOI' (t·ª´ ch·ªëi), 'HOAN_THANH' (ho√†n th√†nh), 'DONG' (ƒë√≥ng),'DANG_XU_LY' (ƒëang x·ª≠ l√Ω), 'DA_XU_LY' (ƒë√£ x·ª≠ l√Ω) ‚Üí N·∫øu ng∆∞·ªùi d√πng vi·∫øt ti·∫øng Vi·ªát nh∆∞ 'T·ª´ ch·ªëi', h√£y √°nh x·∫° sang 'TU_CHOI'.
- TUY·ªÜT ƒê·ªêI KH√îNG ƒë∆∞·ª£c truy v·∫•n tr·ª±c ti·∫øp c√°c c·ªôt ƒë·ªãa ch·ªâ nh∆∞ TINH_THANH_PHO, QUAN_HUYEN, PHUONG_XA b·∫±ng LIKE. Ph·∫£i lu√¥n JOIN v·ªõi b·∫£ng FB_LOCALITY ƒë·ªÉ l·∫•y FULL_NAME. V·ªõi c√°c c·ªôt `PAKH.TINH_THANH_PHO`, `PAKH_QUAN_HUYEN`, `PAKH_PHUONG_XA`, ph·∫£i n·ªëi l·∫°i v√† JOIN v·ªõi b·∫£ng FB_LOCALITY th√¥ng qua quan h·ªá nh∆∞ trong RELATIONS, v√† khi h·ªèi th√¨ thay v√¨ tr·∫£ v·ªÅ 3 c·ªôt m√£ trong PAKH, h√£y tr·∫£ ra FULL_NAME trong FB_LOCALITY v√† ph·∫£i join ƒë·ªß 3 c·ªôt.
- ∆Øu ti√™n d√πng b·∫£ng `PAKH_NOI_DUNG_PHAN_ANH` n·∫øu c·∫ßn truy v·∫•n c√°c tr∆∞·ªùng b√°n c·∫•u tr√∫c ƒë√£ chu·∫©n h√≥a. Khi h·ªèi khu v·ª±c b·ªã l·ªói c·ªßa ph·∫£n √°nh, ∆∞u ti√™n truy v·∫•n c·ªôt KHU_VUC_BI_LOI c·ªßa b·∫£ng PAKH_NOI_DUNG_PHAN_ANH, ngo√†i ra c√≥ th·ªÉ tr·∫£ v·ªÅ t√™n ƒë·ªãa danh ƒë·∫ßy ƒë·ªß truy v·∫•n t·ª´ c√°c c·ªôt trong PAKH nh∆∞ng ƒë√£ li√™n k·∫øt v·ªõi FB_LOCALITY ƒë·ªÉ l·∫•y t√™n thay v√¨ m√£ khu v·ª±c. H·ªèi g√¨ tr·∫£ l·ªùi ƒë√≥, ƒë·ª´ng ƒë∆∞a ra th·ª´a th√¥ng tin.
 {previous_error if previous_error else ''}
- N·∫øu c√≥ l·ªói tr∆∞·ªõc ƒë√≥, s·ª≠a l·ªói v√† t·∫°o l·∫°i c√¢u SQL ch√≠nh x√°c.
- N·∫øu ch·ªâ h·ªèi "bao nhi√™u" m√† kh√¥ng ƒë·ªÅ c·∫≠p "li·ªát k√™", **ch·ªâ c·∫ßn tr·∫£ v·ªÅ k·∫øt qu·∫£ t√≠nh**.
- **QUAN TR·ªåNG V·ªöI C√ÇU H·ªéI TI·∫æP N·ªêI:** N·∫øu c√¢u h·ªèi hi·ªán t·∫°i l√† c√¢u h·ªèi ti·∫øp n·ªëi v√† c√≥ c√°c c√° nh√¢n ƒë√£ ƒë∆∞·ª£c x√°c ƒë·ªãnh tr∆∞·ªõc ƒë√≥ ({", ".join(self.last_user_subjects) if self.last_user_subjects else 'kh√¥ng c√≥'}), h√£y ∆∞u ti√™n tr·∫£ l·ªùi cho c√°c c√° nh√¢n ƒë√≥, tr·ª´ khi c√¢u h·ªèi hi·ªán t·∫°i r√µ r√†ng ch·ªâ ƒë·ªãnh c√° nh√¢n kh√°c ho·∫∑c y√™u c·∫ßu th·ªëng k√™ chung. V√≠ d·ª•, n·∫øu c√¢u tr∆∞·ªõc h·ªèi v·ªÅ 'khainx' v√† 'duc.vole', c√¢u sau h·ªèi 's·ªë l∆∞·ª£ng ph·∫£n √°nh c·ªßa m·ªói c√° nh√¢n ƒë√≥' th√¨ ph·∫£i hi·ªÉu l√† 'khainx' v√† 'duc.vole'.

# RELATIONS
{self._format_relations_for_prompt()}

# Quan tr·ªçng
- TUY·ªÜT ƒê·ªêI KH√îNG B·ªäA RA t√™n b·∫£ng ho·∫∑c t√™n c·ªôt KH√îNG c√≥ trong SCHEMA.
- Lu√¥n ∆∞u ti√™n d√πng ƒë√∫ng SELECT g·ª£i √Ω trong SCHEMA n·∫øu c√≥.
- N·∫øu kh√¥ng ƒë·ªß th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi, h√£y th√¥ng b√°o r√µ thay v√¨ ƒëo√°n.
- TUY·ªÜT ƒê·ªêI KH√îNG JOIN qu√° nhi·ªÅu b·∫£ng khi c√¢u h·ªèi kh√¥ng y√™u c·∫ßu nhi·ªÅu th√¥ng tin nh∆∞ v·∫≠y

C√¢u h·ªèi:
{question_resolved}

SQL:
"""
                sql_raw = self._invoke_model(prompt_text)
                self._update_context_from_sql(sql_raw)
                match = re.findall(r"CA_NHAN\s*=\s*'([^']+)'", sql_raw)
                if match:
                    for subject in match:
                        self.last_user_subjects.add(subject)
                        logger.info(f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t last_user_subjects t·ª´ SQL: {subject}")

                return sql_raw

            except Exception as e:
                logger.error(f"L·ªói sinh SQL (l·∫ßn th·ª≠ {attempt + 1}): {e}")
                if attempt < retries - 1:
                    previous_error = str(e)
                    continue
                raise
    def generate_and_execute_sql(self, question: str, is_follow_up: bool = False, previous_error: str = None, retries: int = 2, force_no_cache: bool = False, execute_fn=None):
        sql_raw = self.generate_sql(question, is_follow_up, previous_error, retries, force_no_cache)
        self.last_sql = sql_raw

        if execute_fn:
            try:
                result = execute_fn(sql_raw)
                key = question.strip().lower()
                if self.last_user_subjects:
                    key += "__" + "__".join(sorted(sub.lower() for sub in self.last_user_subjects))

                self.sql_cache[key] = sql_raw
                logger.info(f"[CACHE] ƒê√£ l∆∞u SQL v√†o cache sau khi execute th√†nh c√¥ng cho c√¢u h·ªèi: {question}")

                return result
            except Exception as e:
                logger.error(f"L·ªói khi execute SQL: {e}")
                return {"error": str(e)}
        else:
            logger.warning("Ch∆∞a truy·ªÅn h√†m execute_fn v√†o generate_and_execute_sql.")
            return None

    def _invoke_model(self, prompt_text: str, retries: int = 2) -> str:
        attempt = 0
        while attempt < retries:
            try:
                if self.engine == "openai":
                    from langchain.prompts import PromptTemplate
                    from langchain.chains import LLMChain

                    sql_prompt = PromptTemplate(input_variables=["input"], template="{input}")
                    sql_chain = LLMChain(llm=self.llm, prompt=sql_prompt)
                    return sql_chain.run(prompt_text).strip()

                elif self.engine == "gemini":
                    return self.model.generate_content(prompt_text).text.strip()

            except Exception as e:
                error_msg = str(e).lower()
                if "rate limit" in error_msg:
                    attempt += 1
                    logger.warning(f"OpenAI rate-limit, retrying attempt {attempt}/{retries}...")
                    if attempt >= retries:
                        logger.warning("OpenAI v·∫´n rate-limit sau retry, fallback Gemini.")
                        self._fallback_to_gemini()
                        return self._invoke_model(prompt_text)
                else:
                    raise

    def _postprocess_sql(self, sql_raw: str, schema_text: str) -> str:
        return sql_raw.strip()

    def _extract_year_from_question(self):
        if self.last_question:
            match = re.search(r"\b(20[2-3][0-9])\b", self.last_question)
            if match:
                return match.group(1)
        return None

    def _extract_nhom_nguyen_nhan_from_question(self):
        if self.last_question:
            match = re.search(r"nh√≥m nguy√™n nh√¢n\s*(\d+)", self.last_question, re.IGNORECASE)
            if match:
                return match.group(1)
        return ""

    # --- Tr·∫£ k·∫øt qu·∫£ lu√¥n ---
    def format_result_for_user(self, result: dict, filter_link: str = None) -> str:
        target = result.get("details") or result if "rows" in result else None
        if not target or not target.get("rows"):
            if "error" in result:
                return f"‚ùå L·ªói SQL: {result['error']}"
            elif "message" in result:
                return f"‚úÖ {result['message']}"
            return "‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu."

        columns = target["columns"]
        rows = target["rows"]
        q = (self.last_question or "").lower()

        is_list_query = any(kw in q for kw in [
            "li·ªát k√™", "danh s√°ch", "list", "show", "c√°c pa", "c√°c ph·∫£n √°nh", "xem chi ti·∫øt", "chi ti·∫øt c√°c ph·∫£n √°nh"
        ])
        is_statistical = any(kw in q for kw in [
            "theo t·ª´ng nh√≥m", "theo t·ª´ng lo·∫°i", "th·ªëng k√™", "group by", "t·ª´ng nh√≥m", "t·ª´ng lo·∫°i", "m·ªói nh√≥m", "m·ªói lo·∫°i"
        ])

        # ‚úÖ X·ª≠ l√Ω th·ªëng k√™ nh√≥m
        nhom_keys = [
            "nhom_pa", "ten_nhom_pa", "nhomphananh", "tennhomphananh",
            "nhom_nguyen_nhan", "ten_nhom_nguyen_nhan"
        ]
        nhom_col = next((col for col in columns if col.lower() in nhom_keys), None)
        if (
            any(kw in q for kw in [
                "theo t·ª´ng nh√≥m ph·∫£n √°nh", "m·ªói nh√≥m ph·∫£n √°nh", "group by nh√≥m ph·∫£n √°nh",
                "theo t·ª´ng nh√≥m nguy√™n nh√¢n", "m·ªói nh√≥m nguy√™n nh√¢n", "group by nh√≥m nguy√™n nh√¢n"
            ])
            or nhom_col
        ):
            if nhom_col:
                nhom_idx = columns.index(nhom_col)
                lines = []
                for row in rows:
                    nhom_val = row[nhom_idx]
                    context_nhom = {
                        "year": self._extract_year_from_question() or "2024",
                        "nhomPa": nhom_val if "nhom" in nhom_col.lower() else "",
                        "nhomNguyenNhan": nhom_val if "nguyen_nhan" in nhom_col.lower() else "",
                    }
                    params = extract_list_params_from_sql(self.last_sql, context_nhom)
                    link = build_filter_url("http://14.160.91.174:8180/smartw/feedback/list.htm", params)
                    lines.append(f"{nhom_col}: {nhom_val}\nüîó [Xem chi ti·∫øt]({link})")
                return os.linesep.join(lines)

        # ‚úÖ Th·ªëng k√™ t·ªïng h·ª£p (kh√¥ng c√≥ link)
        if is_statistical:
            return os.linesep.join(
                "- " + ", ".join(f"{col}: {val}" for col, val in zip(columns, row))
                for row in rows
            )

        # ‚úÖ Li·ªát k√™ chi ti·∫øt
        if is_list_query:
            row_limit = 1
            lines = [
                "- " + ", ".join(f"{col}: {val}" for col, val in zip(columns, row))
                for row in rows[:row_limit]
            ]

            if not filter_link:
                context_common = {"year": self._extract_year_from_question() or "2024"}
                has_xuly = any(keyword in self.last_sql.lower() for keyword in [
                    'ca_nhan', 'phong_ban', 'to_nhom', 'trung_tam'
                ])
                has_diachi = (
                    any(re.search(rf"\b{key}\b", self.last_sql, re.IGNORECASE)
                        for key in ["TINH_THANH_PHO", "QUAN_HUYEN", "MA_TINH", "MA_HUYEN"]) or
                    "FB_LOCALITY" in self.last_sql.upper()
                )

                if has_xuly:
                    context_form = {
                        **context_common,
                        "caNhan": list(self.last_user_subjects)[0] if self.last_user_subjects else "",
                    }
                    params = extract_form_detail_params_from_sql(self.last_sql, context_form)
                    base_url = "http://14.160.91.174:8180/smartw/feedback/form/detail.htm"
                else:
                    base_url = "http://14.160.91.174:8180/smartw/feedback/list.htm"
                    ma_diachi = {}

                    # ∆Øu ti√™n l·∫•y m√£ t·ª´ k·∫øt qu·∫£ truy v·∫•n
                    ma_col_mapping = {
                        "tinhThanhPho": ["ma_tinh", "province", "tinh_thanh_pho"],
                        "quanHuyen": ["ma_huyen", "district", "quan_huyen"],
                    }

                    for key, aliases in ma_col_mapping.items():
                        for alias in aliases:
                            if alias in map(str.lower, columns):
                                idx = next(i for i, col in enumerate(columns) if col.lower() == alias)
                                ma_diachi[key] = rows[0][idx]
                                logger.info(f"[DEBUG] D√πng tr·ª±c ti·∫øp m√£ {key} = {ma_diachi[key]} t·ª´ k·∫øt qu·∫£ truy v·∫•n")
                                break

                    full_name_idx = next(
                        (i for i, col in enumerate(columns) if col.upper() in ["FULL_NAME", "DIA_DIEM_PHAN_ANH"]),
                        None
                    )

                    # Truy v·∫•n ƒë·ªãa ch·ªâ t·ª´ full_name n·∫øu c·∫ßn
                    if not ma_diachi.get("tinhThanhPho") or not ma_diachi.get("quanHuyen"):
                        if full_name_idx is not None:
                            dia_chi = rows[0][full_name_idx]
                            logger.info(f"[DEBUG] ƒê·ªãa ch·ªâ trong k·∫øt qu·∫£ (full_name): {dia_chi}")
                            try:
                                query = """
                                    SELECT province AS tinhThanhPho, district AS quanHuyen
                                    FROM FB_LOCALITY
                                    WHERE full_name = :1
                                    FETCH FIRST 1 ROWS ONLY
                                """
                                with self.db_conn.cursor() as cursor:
                                    cursor.execute(query, [dia_chi])
                                    result_fine = cursor.fetchone()
                                    if result_fine:
                                        col_names = [desc[0].lower() for desc in cursor.description]
                                        for idx, col in enumerate(col_names):
                                            if col in ["tinhthanhpho", "quanhuyen"]:
                                                ma_diachi[col] = result_fine[idx]
                                        logger.info(f"[DEBUG] Mapping FULL_NAME ‚Üí m√£ ƒë·ªãa l√Ω: {ma_diachi}")
                            except Exception as e:
                                logger.error(f"[ERROR] Truy v·∫•n m√£ ƒë·ªãa l√Ω t·ª´ FULL_NAME l·ªói: {e}")
                        else:
                            logger.warning("[DEBUG] Kh√¥ng t√¨m th·∫•y c·ªôt ƒë·ªãa ch·ªâ ph√π h·ª£p trong k·∫øt qu·∫£.")

                    # Fallback: t·ª´ c√¢u h·ªèi
                    if not ma_diachi.get("tinhThanhPho") or not ma_diachi.get("quanHuyen"):
                        dia_chi_cau_hoi = self._extract_dia_chi_from_question(self.last_question or "")
                        logger.info(f"[DEBUG] ƒê·ªãa ch·ªâ t·ª´ c√¢u h·ªèi: {dia_chi_cau_hoi}")
                        fuzzy_result = self.get_ma_dia_chi_fuzzy(dia_chi_cau_hoi)
                        if fuzzy_result:
                            logger.info(f"[DEBUG] Mapping ƒë·ªãa ch·ªâ t·ª´ c√¢u h·ªèi ‚Üí m√£ ƒë·ªãa l√Ω: {fuzzy_result}")
                            ma_diachi.update(fuzzy_result)

                    if ma_diachi:
                        context_common.update(ma_diachi)
                        # üîÅ Normalize key tr∆∞·ªõc khi truy·ªÅn
                    if "tinhthanhpho" in context_common:
                        context_common["tinhThanhPho"] = context_common.pop("tinhthanhpho")
                    if "quanhuyen" in context_common:
                        context_common["quanHuyen"] = context_common.pop("quanhuyen")


                    params = extract_list_params_from_sql(self.last_sql, context_common)
                    logger.info(f"[DEBUG] context_common: {context_common}")
                    logger.info(f"[DEBUG] params for build_filter_url: {params}")
                    filter_link = build_filter_url(base_url, params)

            if filter_link:
                lines.append(f"\nüîó [Xem to√†n b·ªô danh s√°ch t·∫°i ƒë√¢y]({filter_link})")
            return os.linesep.join(lines)


 
    def clear_memory(self):
        if self.engine == "openai":
            self.memory.clear()
        else:
            self.memory = []
        logger.info("ƒê√£ x√≥a b·ªô nh·ªõ h·ªôi tho·∫°i.")

    def clear_specific_cache(self, question: str):
        key = question.strip().lower()
        if self.last_user_subjects:
            key += "__" + "__".join(sorted(sub.lower() for sub in self.last_user_subjects))

        if key in self.sql_cache:
            del self.sql_cache[key]
            logger.info(f"ƒê√£ x√≥a SQL cache cho c√¢u h·ªèi: {question}")

    def clear_cache(self):
        self.sql_cache.clear()
        self.schema_cache.clear()
        logger.info("ƒê√£ x√≥a SQL cache v√† schema cache.")

    def clear_all(self):
        self.clear_cache()
        self.clear_memory()
        self.last_context = {}
        logger.info("ƒê√£ x√≥a to√†n b·ªô b·ªô nh·ªõ v√† cache.")

    def format_result_context(self, result: dict) -> str:
        if "rows" in result and result["rows"]:
            header = ", ".join(result["columns"])
            content = os.linesep.join(
                "- " + ", ".join(f"{col}: {val}" for col, val in zip(result["columns"], row))
                for row in result["rows"][:5]
            )
            return f"{header}{os.linesep}{content}"
        elif "error" in result:
            return f"L·ªói: {result['error']}"
        return "Kh√¥ng c√≥ d·ªØ li·ªáu."