import os
import re
import logging
import json
import unidecode
from typing import List, Dict
from dotenv import load_dotenv
from groq import Groq

from utils.sql_planner import SQLPlanner
from utils.column_mapper import extract_tables_and_columns, generate_column_mapping_hint
from utils.schema_loader import TABLE_KEYWORDS, extract_relevant_tables
from utils.relation_loader import load_relations
from db_utils import get_connection

# --- Setup m√¥i tr∆∞·ªùng & logger ---
load_dotenv()
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

LLM_ENGINE = os.getenv("LLM_ENGINE", "openai").lower()
conn = get_connection()

FORM_DETAIL_PARAMS = [
    "type", "trungTam", "phongBan", "toNhom", "caNhan", "loaiPa", "loaiThueBao",
    "nguyenNhan", "caTruc", "fo_bo", "tinh", "ctdv", "day", "today", "week",
    "month", "quarter", "year", "kpiName", "quanHuyen", "level", "nhomNguyenNhan"
]
LIST_PARAMS = [
    "tab", "id", "year", "tinhThanhPho", "quanHuyen", "is_ticket", "phong", "to", "caTruc",
    "dept_xuly", "dept", "ttTicket", "nhomPa", "loaiPa", "fo_bo", "loaiThueBao", "is_dongpa", "ctdv"
]

def normalize(text):
    return unidecode.unidecode(text).replace(' ', '_').replace('-', '_').lower()
def _clean_sql_for_param_extraction(sql: str) -> str:
    """Lo·∫°i b·ªè LOWER(), UPPER(), TRIM()... quanh t√™n c·ªôt ƒë·ªÉ regex b·∫Øt d·ªÖ h∆°n"""
    sql_clean = re.sub(r"\bLOWER\s*\(\s*([A-Z_]+)\s*\)", r"\1", sql, flags=re.IGNORECASE)
    sql_clean = re.sub(r"\bUPPER\s*\(\s*([A-Z_]+)\s*\)", r"\1", sql_clean, flags=re.IGNORECASE)
    sql_clean = re.sub(r"\bTRIM\s*\(\s*([A-Z_]+)\s*\)", r"\1", sql_clean, flags=re.IGNORECASE)
    return sql_clean

def extract_form_detail_params_from_sql(sql: str, context: dict = None) -> Dict[str, str]:
    sql = _clean_sql_for_param_extraction(sql)
    default_values = {k: "" for k in FORM_DETAIL_PARAMS}
    default_values.update({
        "type": "year",
        "today": "undefined",
        "week": "undefined",
        "month": "undefined",
        "quarter": "undefined",
    })
    params = default_values.copy()
    regex_map = {
        "trungTam": r"TRUNG_TAM\s*=\s*'([^']+)'",
        "phongBan": r"PHONG_BAN\s*=\s*'([^']+)'",
        "toNhom": r"TO_NHOM\s*=\s*'([^']+)'",
        "caNhan": r"(?:\b\w+\.)?CA_NHAN\s*=\s*'([^']+)'",
        "loaiPa": r"LOAI_PA\s*=\s*'([^']+)'",
        "loaiThueBao": r"LOAI_THUE_BAO\s*=\s*'([^']+)'",
        "nguyenNhan": r"NGUYEN_NHAN\s*=\s*'([^']+)'",
        "caTruc": r"CA_TRUC\s*=\s*'([^']+)'",
        "fo_bo": r"FO_BO\s*=\s*'([^']+)'",
        "tinh": r"TINH\s*=\s*'([^']+)'",
        "ctdv": r"CTDV\s*=\s*'([^']+)'",
        "day": r"DAY\s*=\s*'([^']+)'",
        "week": r"WEEK\s*=\s*'([^']+)'",
        "month": r"MONTH\s*=\s*'([^']+)'",
        "quarter": r"QUARTER\s*=\s*'([^']+)'",
        "year": r"YEAR\s*=\s*'([^']+)'",
        "kpiName": r"KPI_NAME\s*=\s*'([^']+)'",
        "quanHuyen": r"QUAN_HUYEN\s*=\s*'([^']+)'",
        "level": r"LEVEL\s*=\s*'([^']+)'",
        "nhomNguyenNhan": r"NHOM_NGUYEN_NHAN\s*=\s*'([^']+)'",
    }
    for key, regex in regex_map.items():
        match = re.search(regex, sql, re.IGNORECASE)
        if match:
            params[key] = match.group(1)
    if context:
        for key in params:
            if key in context and context[key]:
                params[key] = context[key]
    if not params["level"]:
        if "PAKH_CA_NHAN" in sql.upper():
            params["level"] = "ca_nhan"
        elif "PAKH_TO_NHOM" in sql.upper():
            params["level"] = "to_nhom"
        elif "PAKH_PHONG_BAN" in sql.upper():
            params["level"] = "phong_ban"
        elif "PAKH_TRUNG_TAM" in sql.upper():
            params["level"] = "trung_tam"
    if not params["year"]:
        match = re.search(r"\b(20[2-3][0-9])\b", sql)
        if match:
            params["year"] = match.group(1)
        elif context and "year" in context and context["year"]:
            params["year"] = context["year"]
    if not params["kpiName"]:
            trang_thai_map = {
                "TU_CHOI": "soPaTuChoi",
                "DONG": "soPaDong",
                "DANG_XU_LY": "soPaDangXlDh",
                "DA_XU_LY": "soPaDaXlyDh",
                "HOAN_THANH": "soPaHoanThanh",
            }
            match = re.search(r"TRANG_THAI\s*=\s*'([^']+)'", sql, re.IGNORECASE)
            if match:
                trang_thai = match.group(1).upper()
                if trang_thai in trang_thai_map:
                    params["kpiName"] = trang_thai_map[trang_thai]
    
    return params

def extract_list_params_from_sql(sql: str, context: dict = None) -> Dict[str, str]:
    sql = _clean_sql_for_param_extraction(sql)
    default_values = {k: "" for k in LIST_PARAMS}
    default_values.update({
        "tab": "year",
        "is_ticket": "Y%2CN",
        "is_dongpa": "DANG_XU_LY%2CDA_XU_LY",
    })
    params = default_values.copy()
    regex_map = {
        "tab": r"tab\s*=\s*'([^']+)'",
        "id": r"id\s*=\s*'([^']+)'",
        "year": r"year\s*=\s*'([^']+)'",
        "tinhThanhPho": r"TINH_THANH_PHO\s*=\s*'([^']+)'",
        "quanHuyen": r"QUAN_HUYEN\s*=\s*'([^']+)'",
        "is_ticket": r"is_ticket\s*=\s*'([^']+)'",
        "phong": r"PHONG\s*=\s*'([^']+)'",
        "to": r"TO_NHOM\s*=\s*'([^']+)'",
        "caTruc": r"CA_TRUC\s*=\s*'([^']+)'",
        "dept_xuly": r"DEPT_XULY\s*=\s*'([^']+)'",
        "dept": r"DEPT\s*=\s*'([^']+)'",
        "ttTicket": r"TT_TICKET\s*=\s*'([^']+)'",
        "nhomPa": r"NHOM_PA\s*=\s*'([^']+)'",
        "loaiPa": r"LOAI_PA\s*=\s*'([^']+)'",
        "fo_bo": r"FO_BO\s*=\s*'([^']+)'",
        "loaiThueBao": r"LOAI_THUE_BAO\s*=\s*'([^']+)'",
        "is_dongpa": r"is_dongpa\s*=\s*'([^']+)'",
        "ctdv": r"CTDV\s*=\s*'([^']+)'"
    }
    for key, regex in regex_map.items():
        match = re.search(regex, sql, re.IGNORECASE)
        if match:
            params[key] = match.group(1)
    if context:
        for key in params:
            if key in context and context[key]:
                params[key] = context[key]
    return params

def build_filter_url(base: str, params: Dict[str, str]) -> str:
    query = "&".join(f"{k}={v}" for k, v in params.items())
    return f"{base}?{query}"

# --- L·ªõp Cache cho Schema ---
class SchemaCache:
    def __init__(self):
        self.cache = {}
    def get_schema(self, table_name):
        if table_name not in self.cache:
            from utils.schema_loader import load_schema
            self.cache[table_name] = load_schema(table_name)
        return self.cache[table_name]
    def clear(self):
        self.cache.clear()
        logger.info("ƒê√£ x√≥a schema cache.")
        
# --- L·ªõp ch√≠nh HybridSQLCoder ---
class HybridSQLCoder:
    def __init__(self, db_conn=None):
        self.engine = LLM_ENGINE
        self.sql_cache = {}
        self.schema_cache = SchemaCache()
        self.sql_cache = {}
        self.context = {}
        self.context_filters = None
        self.groq_client = None
        self._init_groq()

        self.relations, self.concat_mappings = load_relations()
        self.context = {
            "filters": {},
            "subjects": set(),
            "last_question": None,
            "last_sql": None,
            "last_result": None
        }
        self.db_conn = db_conn
        BASE_DIR = os.path.dirname(os.path.abspath(__file__))
        EXAMPLES_FILE = os.path.join(BASE_DIR, "utils", "sql_examples.json")
        self.sql_examples = self._load_sql_examples(EXAMPLES_FILE)

        if self.engine == "openai":
            self._init_openai()
        else:
            self._init_gemini()

    def _init_openai(self):
        from langchain_openai import ChatOpenAI
        from langchain.memory import ConversationSummaryMemory

        self.model_name = os.getenv("OPENAI_MODEL", "gpt-4")
        self.llm = ChatOpenAI(model=self.model_name, temperature=0)
        self.memory = ConversationSummaryMemory(
            llm=self.llm, memory_key="chat_history", return_messages=True
        )

    def _init_gemini(self):
        import google.generativeai as genai

        api_key = os.getenv("GEMINI_API_KEY")
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel("models/gemini-2.5-flash", generation_config={"temperature": 0})
        self.memory = []  # list of (user_message, ai_message)

    def _init_groq(self):
        api_key = os.getenv("GROQ_API_KEY")
        if not api_key:
            raise ValueError("Thi·∫øu GROQ_API_KEY trong file .env")
        self.groq_client = Groq(api_key=api_key)
        
    def _fallback_to_gemini(self):
        logger.warning("OpenAI g·∫∑p l·ªói, chuy·ªÉn sang Gemini.")
        self._init_gemini()
        self.engine = "gemini"
    def _extract_where_conditions(self, sql: str) -> str:
        """
        Tr√≠ch xu·∫•t ph·∫ßn ƒëi·ªÅu ki·ªán WHERE t·ª´ c√¢u SQL.
        Tr·∫£ v·ªÅ chu·ªói ƒëi·ªÅu ki·ªán (kh√¥ng bao g·ªìm ch·ªØ WHERE).
        """
        # B·ªè ; ·ªü cu·ªëi ƒë·ªÉ tr√°nh match sai
        sql = sql.strip().rstrip(";")
        
        # T√¨m WHERE ... (ƒë·∫øn h·∫øt ho·∫∑c tr∆∞·ªõc GROUP/ORDER)
        match = re.search(r"\bWHERE\b\s+(.*?)(?:\bGROUP\b|\bORDER\b|$)", sql, re.IGNORECASE | re.DOTALL)
        if match:
            return match.group(1).strip()
        return ""
    # --- Helper ---
    def _get_last_n_user_questions(self, n=5):
        if self.engine == "openai" and hasattr(self.memory, "chat_memory"):
            return [
                msg.content for msg in reversed(self.memory.chat_memory.messages)
                if msg.type == "human"
            ][:n][::-1]
        elif self.engine == "gemini":
            return [user for user, _ in self.memory[-n:]][::-1]
        return []
    def reset_context(self):
        self.context = {
            "filters": {},
            "subjects": set(),
            "last_question": None,
            "last_sql": None,
            "last_result": None
        }
        logger.info("[CONTEXT] ƒê√£ reset to√†n b·ªô context.")
    def update_context(self, sql=None, result=None, question=None):
        """C·∫≠p nh·∫≠t context t·ª´ SQL, k·∫øt qu·∫£ truy v·∫•n v√† c√¢u h·ªèi."""
        if question is not None:
            self.context["last_question"] = question
        if sql is not None:
            self.context["last_sql"] = sql
        if result is not None:
            self.context["last_result"] = result

    def get_ma_dia_chi_fuzzy(self, dia_chi_text: str) -> dict:
        if not self.db_conn:
            logger.warning("[WARN] DB connection is None. Kh√¥ng th·ªÉ truy v·∫•n m√£ ƒë·ªãa l√Ω.")
            return {}
        
        import unidecode

        # B·ªè d·∫•u, lowercase, lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát
        text = unidecode.unidecode(dia_chi_text).lower()
        text = re.sub(r"[^\w\s]", " ", text)
        keywords = [kw.strip() for kw in text.split() if kw.strip()]
        if not keywords:
            return {}

        # T·∫°o ƒëi·ªÅu ki·ªán WHERE b·∫±ng LIKE
        conditions = []
        params = []
        for i, kw in enumerate(keywords):
            conditions.append(f"LOWER(full_name) LIKE :{i+1}")
            params.append(f"%{kw}%")
        
        where_clause = " AND ".join(conditions)
        query = f"""
    SELECT province AS tinhThanhPho, district AS quanHuyen 
    FROM FB_LOCALITY 
    WHERE {where_clause}
    ORDER BY LENGTH(full_name) DESC
    FETCH FIRST 1 ROWS ONLY
"""


        try:
            with self.db_conn.cursor() as cursor:
                cursor.execute(query, params)
                row = cursor.fetchone()
                if row:
                    result = {
                        "tinhThanhPho": row[0],
                        "quanHuyen": row[1]
                    }
                    logger.info(f"[DEBUG] Mapping ƒë·ªãa ch·ªâ '{dia_chi_text}' -> {result}")
                    return result
        except Exception as e:
            logger.error(f"L·ªói truy v·∫•n m√£ ƒë·ªãa l√Ω: {e}")
        return {}

    def _extract_dia_chi_from_question(self, question: str) -> str:
        if not question:
            return ""

        import unidecode
        q_norm = unidecode.unidecode(question).lower()

        # Chu·∫©n h√≥a c√°c d·∫°ng vi·∫øt t·∫Øt
        q_norm = re.sub(r"\bq\.?\s", "quan ", q_norm)  # Q Ho√†ng Mai, Q. Ho√†ng Mai ‚Üí quan Ho√†ng Mai
        q_norm = re.sub(r"\bh\.?\s", "huyen ", q_norm) # H H√≥c M√¥n, H. H√≥c M√¥n ‚Üí huyen H√≥c M√¥n
        q_norm = re.sub(r"\btp\.?\s", "thanh pho ", q_norm) # TP, TP. ‚Üí thanh pho
        q_norm = re.sub(r"\btp\b", "thanh pho", q_norm)

        # Regex nh·∫≠n di·ªán ƒë·∫ßy ƒë·ªß c√°c lo·∫°i ƒë·ªãa ch·ªâ
        patterns = [
            r"(?:tai|o|thuoc|dia ban|khu vuc|tinh|thanh pho|quan|huyen|xa|phuong)\s+([^\.,;?\n]+)",
            r"(quan|huyen|phuong|xa|thi xa|tp|thanh pho)\s+\w+(?:\s+\w+)?"
        ]

        for pattern in patterns:
            match = re.search(pattern, q_norm, re.IGNORECASE)
            if match:
                return match.group(0).strip()

        # fallback: l·∫•y 3 t·ª´ cu·ªëi
        return " ".join(q_norm.strip().split()[-3:])

    def _detect_new_filter(self, question: str) -> bool:
        # N·∫øu c√¢u h·ªèi c√≥ t·ª´ kh√≥a v·ªÅ ph√≤ng ban, t·ªï nh√≥m, trung t√¢m, ƒë·ªãa b√†n, nh√≥m, lo·∫°i...
        keywords = [
            "ph√≤ng", "ph√≤ng ban", "t·ªï", "t·ªï nh√≥m", "trung t√¢m", "ƒë·ªãa b√†n", "t·ªânh", "huy·ªán",
            "nh√≥m ph·∫£n √°nh", "lo·∫°i ph·∫£n √°nh", "c√° nh√¢n", "s·ªë thu√™ bao", "m√£ t·ªânh", "m√£ huy·ªán"
        ]
        q = question.lower()
        return any(kw in q for kw in keywords)

    def _resolve_follow_up_question(self, question: str, is_follow_up: bool = False) -> str:
        if not is_follow_up:
            return question
        last_q = self.context["last_question"] or ""
        if not last_q:
            return question
        prompt = f"""
B·∫°n l√† tr·ª£ l√Ω ng√¥n ng·ªØ.
C√¢u h·ªèi tr∆∞·ªõc: {last_q}
C√¢u h·ªèi hi·ªán t·∫°i: {question}
H√£y vi·∫øt l·∫°i c√¢u h·ªèi hi·ªán t·∫°i th√†nh c√¢u h·ªèi ƒë·∫ßy ƒë·ªß, r√µ nghƒ©a.
"""
        resolved = self._invoke_model(prompt)
        return resolved.strip() if resolved else question

    def _detect_new_filter(self, question: str) -> bool:
        keywords = [
            "ph√≤ng", "ph√≤ng ban", "t·ªï", "t·ªï nh√≥m", "trung t√¢m", "ƒë·ªãa b√†n", "t·ªânh", "huy·ªán",
            "nh√≥m ph·∫£n √°nh", "lo·∫°i ph·∫£n √°nh", "c√° nh√¢n", "s·ªë thu√™ bao", "m√£ t·ªânh", "m√£ huy·ªán"
        ]
        return any(kw in question.lower() for kw in keywords)

    def _format_relations_for_prompt(self):
        return os.linesep.join(
            f"- {table}.{col} ‚Üí {ref['ref_table']}.{ref['ref_column']}"
            for table, cols in self.relations.items()
            for col, ref in cols.items()
        )

    def _generate_sum_hint(self) -> str:
        hints = [
            "- ƒê·ªëi v·ªõi c√°c b·∫£ng SLA (PAKH_SLA_*):",
            "   - Lu√¥n d√πng SUM() v·ªõi c√°c c·ªôt t·ªïng h·ª£p nh∆∞ SO_PA_NHAN, SO_PA_DA_XL, SO_PA_QH.",
            "   - √Ånh x·∫° t·ª´ kh√≥a trong c√¢u h·ªèi sang c·ªôt t·ªïng h·ª£p:",
            "     - 'qu√° h·∫°n' ‚Üí SUM(SO_PA_QH) (S·ªë ph·∫£n √°nh qu√° h·∫°n)",
            "     - 'ƒë√£ x·ª≠ l√Ω ƒë√∫ng h·∫°n' ‚Üí SUM(SO_PA_DA_XL_DH)",
            "     - 'ƒëang x·ª≠ l√Ω ƒë√∫ng h·∫°n' ‚Üí SUM(SO_PA_DANG_XL_DH)",
            "     - 't·ª´ ch·ªëi' ho·∫∑c 'b·ªã t·ª´ ch·ªëi' ‚Üí SUM(SO_PA_TU_CHOI)",
            "     - 't·ªïng th·ªùi gian x·ª≠ l√Ω' ‚Üí SUM(TONG_TG_XL) / 60 (ƒë·ªïi sang gi·ªù)",
            "     - 'th·ªùi gian trung b√¨nh x·ª≠ l√Ω' ‚Üí  ROUND(SUM(TONG_TG_XL) / NULLIF(SUM(SO_PA_DA_XL), 0), 2) AS TB_PHUT, TRUNC(SUM(TONG_TG_XL) / NULLIF(SUM(SO_PA_DA_XL), 0) / 60) AS GIO, MOD(ROUND(SUM(TONG_TG_XL) / NULLIF(SUM(SO_PA_DA_XL), 0)), 60) AS PHUT",
            "   - TUY·ªÜT ƒê·ªêI KH√îNG JOIN b·∫£ng SLA tr·ª±c ti·∫øp v·ªõi PAKH ho·∫∑c PAKH_CA_NHAN cho c√°c truy v·∫•n th·ªëng k√™.",
            "- C√¥ng th·ª©c t√≠nh t·ª∑ l·ªá x·ª≠ l√Ω ƒë√∫ng h·∫°n: (SUM(SO_PA_DA_XL_DH) + SUM(SO_PA_DANG_XL_DH)) / NULLIF(SUM(SO_PA_NHAN), 0) * 100.",
            "- D√πng b·∫£ng SLA ph√π h·ª£p v·ªõi ng·ªØ c·∫£nh c√° nh√¢n, t·ªï nh√≥m, ph√≤ng ban, trung t√¢m",
            "- L·∫•y t√™n c√° nh√¢n trong c·ªôt CA_NHAN"
        ]
        return os.linesep.join(hints)

    def _load_sql_examples(self, file_path: str) -> List[Dict[str, str]]:
        if os.path.exists(file_path):
            with open(file_path, "r", encoding="utf-8") as f:
                return json.load(f)
        logger.warning(f"File v√≠ d·ª• SQL kh√¥ng t√¨m th·∫•y: {file_path}")
        return []

    def _select_relevant_examples(self, question: str, num_examples: int = 3) -> str:
        question_lower = question.lower()
        question_words = set(question_lower.split())
        scored_examples = []
        for example in self.sql_examples:
            score = len(question_words.intersection(set(example["question"].lower().split())))
            if score > 0:
                scored_examples.append((score, example))
        scored_examples.sort(key=lambda x: x[0], reverse=True)
        relevant_examples = [ex for _, ex in scored_examples[:num_examples]]
        example_str = ""
        if relevant_examples:
            example_str += "\n---EXAMPLES START---\n"
            for ex in relevant_examples:
                example_str += f"C√¢u h·ªèi: {ex['question']}\nSQL:\n```sql\n{ex['sql']}\n```\n\n"
            example_str += "---EXAMPLES END---\n\n"
        return example_str

    # --- Sinh SQL ---
    def generate_sql(self, question: str, is_follow_up: bool = False, previous_error: str = None,
                 retries: int = 2, force_no_cache: bool = False, reset: bool = False):
    # Reset n·∫øu l√† c√¢u h·ªèi m·ªõi
        if reset and not is_follow_up:  # <-- ch·ªâ reset khi kh√¥ng ph·∫£i ti·∫øp t·ª•c
            self.reset_context()

        # L∆∞u c√¢u h·ªèi ngay
        self.update_context(question=question)

        key = question.strip().lower()
        if self.context.get("subjects"):
            key += "__" + "__".join(sorted(sub.lower() for sub in self.context["subjects"]))

        # Cache
        if not force_no_cache and key in self.sql_cache:
            logger.info(f"[CACHE] S·ª≠ d·ª•ng SQL cache cho: {question}")
            return self.sql_cache[key]


        for attempt in range(retries):
            try:
                context_for_prompt = ""
                if is_follow_up and all(self.context.get(k) for k in ["last_question", "last_sql", "last_result"]):
                    filter_str = ""
                    if self.context["filters"]:
                        filter_str = "\nC√°c b·ªô l·ªçc ƒë√£ l∆∞u t·ª´ tr∆∞·ªõc: " + json.dumps(self.context["filters"], ensure_ascii=False)
                    context_for_prompt = f"""
                    # Ng·ªØ c·∫£nh h·ªôi tho·∫°i tr∆∞·ªõc ƒë√≥:
                    C√¢u h·ªèi: {self.context['last_question']}
                    SQL: {self.context['last_sql']}
                    K·∫øt qu·∫£: {json.dumps(self.context['last_result'], ensure_ascii=False)}
                    {filter_str}
                    """
                question_resolved = self._resolve_follow_up_question(question, is_follow_up)
                planner = SQLPlanner(self._invoke_model)
                plan_result = planner.plan(question_resolved)
                relevant_tables = plan_result.get("tables", []) or extract_relevant_tables(question_resolved)

                relevant_examples_str = self._select_relevant_examples(question)
                if "FB_LOCALITY" not in relevant_tables:
                    relevant_tables.append("FB_LOCALITY")

                schema_text = os.linesep + os.linesep.join(
                    self.schema_cache.get_schema(tbl) for tbl in relevant_tables if tbl
                )
                if not schema_text:
                    raise ValueError("Kh√¥ng t√¨m th·∫•y schema ph√π h·ª£p.")

                column_mapping_hint = generate_column_mapping_hint(question_resolved)
                
                prompt_text = f"""
{context_for_prompt}
B·∫°n l√† tr·ª£ l√Ω sinh truy v·∫•n SQL cho CSDL Oracle.
# M·ª•c ti√™u
Sinh truy v·∫•n SQL ch√≠nh x√°c. ∆Øu ti√™n tr·∫£ l·ªùi nhanh ch√≥ng, ƒë∆°n gi·∫£n.

{relevant_examples_str}

# √Ånh x·∫° t·ª´ t·ª´ kh√≥a ‚Üí b·∫£ng.c·ªôt
{column_mapping_hint}
        
# SCHEMA
{schema_text}

# H∆∞·ªõng d·∫´n
{self._generate_sum_hint()}
- Ch·∫•p nh·∫≠n c√°c h√¨nh th·ª©c vi·∫øt t·∫Øt, v√≠ d·ª• KHCN - Kh√°ch h√†ng c√° nh√¢n, KHDN - Kh√°ch h√†ng doanh nghi·ªáp, pa - ph·∫£n √°nh, HCM - Th√†nh ph·ªë H·ªì Ch√≠ Minh.
- V·ªõi s·ªë thu√™ bao ng∆∞·ªùi d√πng, trong c∆° s·ªü d·ªØ li·ªáu ƒëang c√≥ ƒë·ªãnh d·∫°ng kh√¥ng c√≥ s·ªë 0 ·ªü ƒë·∫ßu, khi ng∆∞·ªùi d√πng nh·∫≠p c√¢u h·ªèi B·∫ÆT BU·ªòC ch·∫•p nh·∫≠n c·∫£ ki·ªÉu nh·∫≠p C√ì S·ªê 0 v√† KH√îNG c√≥ s·ªë 0.
- V·ªõi nh·ªØng c√¢u li·ªát k√™, ch·ªâ tr·∫£ v·ªÅ c·ªôt PAKH.SO_THUE_BAO v√† PAKH.NOI_DUNG_PHAN_ANH, KH√îNG TR·∫¢ C√ÅC C·ªòT KH√ÅC.
- N·∫øu c√¢u h·ªèi c√≥ y·∫øu t·ªë ƒë·ªãa ch·ªâ, lu√¥n SELECT th√™m c·ªôt m√£ ƒë·ªãa l√Ω (TINH_THANH_PHO, QUAN_HUYEN) ho·∫∑c PROVINCE, DISTRICT t·ª´ b·∫£ng ƒë·ªãa ch·ªâ (nh∆∞ FB_LOCALITY) ƒë·ªÉ ph·ª•c v·ª• mapping.
- ∆ØU TI√äN xem m·ª©c ƒë·ªô t∆∞∆°ng th√≠ch v·ªõi t·ª´ kh√≥a trong table_keywords.json ƒë·ªÉ truy v·∫•n v√† tr·∫£ ra ƒë√∫ng c·ªôt ƒë∆∞·ª£c h·ªèi, kh√¥ng tr·∫£ l·ªùi thi·∫øu hay th·ª´a th√¥ng tin, n·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ n·ªôi dung SIM, g√≥i c∆∞·ªõc, m·∫°ng y·∫øu,... th√¨ th√¥ng tin s·∫Ω ƒë∆∞·ª£c l∆∞u trong b·∫£ng `PAKH_NOI_DUNG_PHAN_ANH`, KH√îNG PH·∫¢I b·∫£ng `PAKH`.
- V·ªõi nh·ªØng c√¢u h·ªèi v·ªÅ s·ªë l∆∞·ª£ng bao nhi√™u B·∫ÆT BU·ªòC d√πng c√°c b·∫£ng PAKH_SLA_* v√¨ c√°c b·∫£ng n√†y ƒë√£ ch·ª©a c√°c s·ªë li·ªáu t·ªïng h·ª£p, KH√îNG C·∫¶N V√Ä KH√îNG ƒê∆Ø·ª¢C PH√âP JOIN v·ªõi b·∫£ng PAKH ho·∫∑c PAKH_CA_NHAN.
- Khi ng∆∞·ªùi d√πng ti·∫øp t·ª•c h·ªèi ‚Äúli·ªát k√™ c√°c ph·∫£n √°nh ƒë√≥‚Äù ‚Üí ph·∫£i chuy·ªÉn sang JOIN PAKH_CA_NHAN v√† PAKH qua ID ƒë·ªÉ l·∫•y ƒë·∫ßy ƒë·ªß th√¥ng tin t·ª´ b·∫£ng PAKH, v√† b·∫Øt bu·ªôc tr·∫£ ra danh s√°ch th√¥ng tin ph·∫£n √°nh t·ª´ b·∫£ng PAKH. 
- V·ªõi c√°c tr∆∞·ªùng m√£ nh∆∞ `LOAI_PHAN_ANH`, `FB_GROUP`, `HIEN_TUONG`, `NGUYEN_NHAN`, `DON_VI_NHAN` c·∫ßn JOIN b·∫£ng t∆∞∆°ng ·ª©ng (FB_TYPE, FB_GROUP, FB_HIEN_TUONG, FB_REASON, FB_DEPARTMENT) ƒë·ªÉ tr·∫£ ra t√™n (`NAME`) thay v√¨ tr·∫£ ra m√£.
- L∆ØU √ù: c·ªôt `TRANG_THAI` ch·ªâ t·ªìn t·∫°i trong c√°c b·∫£ng `PAKH_CA_NHAN`, `PAKH_PHONG_BAN`, `PAKH_TO_NHOM`, `PAKH_TRUNG_TAM` v·ªõi gi√° tr·ªã h·ª£p l·ªá l√† c√°c m√£ vi·∫øt hoa kh√¥ng d·∫•u: 'TU_CHOI' (t·ª´ ch·ªëi), 'HOAN_THANH' (ho√†n th√†nh), 'DONG' (ƒë√≥ng),'DANG_XU_LY' (ƒëang x·ª≠ l√Ω), 'DA_XU_LY' (ƒë√£ x·ª≠ l√Ω) ‚Üí N·∫øu ng∆∞·ªùi d√πng vi·∫øt ti·∫øng Vi·ªát nh∆∞ 'T·ª´ ch·ªëi', h√£y √°nh x·∫° sang 'TU_CHOI'.
- TUY·ªÜT ƒê·ªêI KH√îNG ƒë∆∞·ª£c truy v·∫•n tr·ª±c ti·∫øp c√°c c·ªôt ƒë·ªãa ch·ªâ nh∆∞ TINH_THANH_PHO, QUAN_HUYEN, PHUONG_XA b·∫±ng LIKE. Ph·∫£i lu√¥n JOIN v·ªõi b·∫£ng FB_LOCALITY ƒë·ªÉ l·∫•y FULL_NAME. V·ªõi c√°c c·ªôt `PAKH.TINH_THANH_PHO`, `PAKH_QUAN_HUYEN`, `PAKH_PHUONG_XA`, ph·∫£i n·ªëi l·∫°i v√† JOIN v·ªõi b·∫£ng FB_LOCALITY th√¥ng qua quan h·ªá nh∆∞ trong RELATIONS, v√† khi h·ªèi th√¨ thay v√¨ tr·∫£ v·ªÅ 3 c·ªôt m√£ trong PAKH, h√£y tr·∫£ ra FULL_NAME trong FB_LOCALITY v√† ph·∫£i join ƒë·ªß 3 c·ªôt.
- ∆Øu ti√™n d√πng b·∫£ng `PAKH_NOI_DUNG_PHAN_ANH` n·∫øu c·∫ßn truy v·∫•n c√°c tr∆∞·ªùng b√°n c·∫•u tr√∫c ƒë√£ chu·∫©n h√≥a. Khi h·ªèi khu v·ª±c b·ªã l·ªói c·ªßa ph·∫£n √°nh, ∆∞u ti√™n truy v·∫•n c·ªôt KHU_VUC_BI_LOI c·ªßa b·∫£ng PAKH_NOI_DUNG_PHAN_ANH, ngo√†i ra c√≥ th·ªÉ tr·∫£ v·ªÅ t√™n ƒë·ªãa danh ƒë·∫ßy ƒë·ªß truy v·∫•n t·ª´ c√°c c·ªôt trong PAKH nh∆∞ng ƒë√£ li√™n k·∫øt v·ªõi FB_LOCALITY ƒë·ªÉ l·∫•y t√™n thay v√¨ m√£ khu v·ª±c. H·ªèi g√¨ tr·∫£ l·ªùi ƒë√≥, ƒë·ª´ng ƒë∆∞a ra th·ª´a th√¥ng tin.
 {previous_error if previous_error else ''}
- N·∫øu c√≥ l·ªói tr∆∞·ªõc ƒë√≥, s·ª≠a l·ªói v√† t·∫°o l·∫°i c√¢u SQL ch√≠nh x√°c.
- N·∫øu ch·ªâ h·ªèi "bao nhi√™u" m√† kh√¥ng ƒë·ªÅ c·∫≠p "li·ªát k√™", **ch·ªâ c·∫ßn tr·∫£ v·ªÅ k·∫øt qu·∫£ t√≠nh**.
- N·∫øu c√¢u h·ªèi ti·∫øp n·ªëi, ∆∞u ti√™n context: {self.context["filters"]}

# RELATIONS
{self._format_relations_for_prompt()}

# Quan tr·ªçng
- TUY·ªÜT ƒê·ªêI KH√îNG B·ªäA RA t√™n b·∫£ng ho·∫∑c t√™n c·ªôt KH√îNG c√≥ trong SCHEMA.
- Lu√¥n ∆∞u ti√™n d√πng ƒë√∫ng SELECT g·ª£i √Ω trong SCHEMA n·∫øu c√≥.
- N·∫øu kh√¥ng ƒë·ªß th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi, h√£y th√¥ng b√°o r√µ thay v√¨ ƒëo√°n.
- TUY·ªÜT ƒê·ªêI KH√îNG JOIN qu√° nhi·ªÅu b·∫£ng khi c√¢u h·ªèi kh√¥ng y√™u c·∫ßu nhi·ªÅu th√¥ng tin nh∆∞ v·∫≠y

C√¢u h·ªèi:
{question_resolved}

SQL:
"""
                sql_raw = self._invoke_model(prompt_text)
                self.update_context(sql=sql_raw)
                
                return sql_raw

            except Exception as e:
                logger.error(f"L·ªói sinh SQL (l·∫ßn th·ª≠ {attempt + 1}): {e}")
                if attempt < retries - 1:
                    previous_error = str(e)
                    continue
                raise
    def generate_and_execute_sql(self, question: str, is_follow_up: bool = False, previous_error: str = None,
                             retries: int = 2, force_no_cache: bool = False, execute_fn=None,  reset: bool = False):
        sql_raw = self.generate_sql(question, is_follow_up, previous_error, retries, force_no_cache)

        if execute_fn:
            try:
                result = execute_fn(sql_raw)

                # L∆∞u k·∫øt qu·∫£ v√†o context
                self.update_context(result=result)
                # N·∫øu ch·∫°y th√†nh c√¥ng v√† c√≥ d·ªØ li·ªáu th√¨ l∆∞u ƒëi·ªÅu ki·ªán WHERE
                if result and isinstance(result, dict):
                    extracted_filter = self._extract_where_conditions(sql_raw)
                    if extracted_filter:
                        self.context_filters = extracted_filter
                        logger.info(f"[CONTEXT] L∆∞u filter: {self.context_filters}")

                        # üîπ Parse c√°c tham s·ªë chi ti·∫øt ƒë·ªÉ l∆∞u v√†o context["filters"]
                        form_params = extract_form_detail_params_from_sql(sql_raw, self.context.get("filters", {}))
                        # B·ªè c√°c param r·ªóng
                        form_params = {k: v for k, v in form_params.items() if v}
                        self.context["filters"].update(form_params)
                        logger.info(f"[CONTEXT] C·∫≠p nh·∫≠t filters: {self.context['filters']}")

                # Cache SQL
                key = question.strip().lower()
                if self.context.get("subjects"):
                    key += "__" + "__".join(sorted(sub.lower() for sub in self.context["subjects"]))
                self.sql_cache[key] = sql_raw

                return result
            except Exception as e:
                logger.error(f"L·ªói execute SQL: {e}")
                return {"error": str(e)}
        else:
            logger.warning("Ch∆∞a truy·ªÅn execute_fn v√†o generate_and_execute_sql.")
            return None


    def _invoke_model(self, prompt_text: str, retries: int = 1) -> str:
        attempt = 0
        while attempt < retries:
            try:
                if self.engine == "openai":
                    from langchain.prompts import PromptTemplate
                    from langchain.chains import LLMChain

                    sql_prompt = PromptTemplate(input_variables=["input"], template="{input}")
                    sql_chain = LLMChain(llm=self.llm, prompt=sql_prompt)
                    return sql_chain.run(prompt_text).strip()

                elif self.engine == "gemini":
                    return self.model.generate_content(prompt_text).text.strip()

            except Exception as e:
                error_msg = str(e).lower()
                if "rate limit" in error_msg:
                    attempt += 1
                    logger.warning(f"OpenAI rate-limit, retrying attempt {attempt}/{retries}...")
                    if attempt >= retries:
                        logger.warning("OpenAI v·∫´n rate-limit sau retry, fallback Gemini.")
                        self._fallback_to_gemini()
                        return self._invoke_model(prompt_text)
                else:
                    raise

    def _postprocess_sql(self, sql_raw: str, schema_text: str) -> str:
        return sql_raw.strip()

    def _extract_year_from_question(self):
        if self.context["last_question"]:
            match = re.search(r"\b(20[2-3][0-9])\b", self.context["last_question"], re.IGNORECASE)
            if match:
                return match.group(1)
        return None

    def format_result_for_user(self, result: dict, filter_link: str = None) -> str:
        logger.info(f"[DEBUG] last_question: {self.context.get('last_question')}")
        logger.info(f"[DEBUG] last_sql: {self.context.get('last_sql')}")
        logger.info(f"[DEBUG] last_result: {self.context.get('last_result')}")
        logger.info(f"[DEBUG] filters: {self.context.get('filters')}")

        def _safe_join(items):
            return os.linesep.join("" if item is None else str(item) for item in items)

        # --- Fix to√°n t·ª≠ ∆∞u ti√™n ---
        target = result.get("details") or (result if "rows" in result else None)

        # --- Debug ƒë·ªÉ xem target th·ª±c t·∫ø l√† g√¨ ---
        logger.info(f"[DEBUG] target type: {type(target)}, target: {target}")

        if not target or not target.get("rows"):
            if "error" in result:
                return f"‚ùå L·ªói SQL: {result['error']}"
            elif "message" in result:
                return f"‚úÖ {result['message']}"
            return "‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu."

        columns = target["columns"]
        rows = target["rows"]

        logger.info(f"[DEBUG] columns: {columns}")
        logger.info(f"[DEBUG] rows: {rows}")

        q = (self.context["last_question"] or "").lower()

        is_statistical, is_info_query, is_list_query = self.detect_query_type(self.context["last_question"], target)

        # --- X·ª≠ l√Ω th·ªëng k√™ ---
        if is_statistical:
            logger.info("[DEBUG] ƒêang x·ª≠ l√Ω d·∫°ng th·ªëng k√™")
            lines = []
            if len(columns) == 1 and len(rows) == 1:
                val = rows[0][0]
                logger.info(f"[DEBUG] Gi√° tr·ªã th·ªëng k√™ ƒë∆°n: {val}")
                return str(val if val is not None else 0)

            for row in rows:
                safe_parts = []
                for col, val in zip(columns, row):
                    col_str = str(col) if col is not None else ""
                    val_str = str(val) if val is not None else "0"
                    safe_parts.append(f"{col_str}: {val_str}")
                lines.append(", ".join(safe_parts) if safe_parts else "")

            logger.info(f"[DEBUG] lines tr∆∞·ªõc khi join: {lines}")
            return _safe_join(lines)

        # --- Truy v·∫•n th√¥ng tin ---
        if is_info_query:
            # N·∫øu ch·ªâ c√≥ 1 c·ªôt ‚Üí tr·∫£ danh s√°ch gi√° tr·ªã
            if len(columns) == 1:
                values = [str(row[0]) if row[0] is not None else "N/A" for row in rows]
                return _safe_join(values)

            # Nhi·ªÅu c·ªôt ‚Üí tr·∫£ d·∫°ng "col: value"
            lines = []
            for row in rows:
                formatted_row = ", ".join(
                    f"{col}: {str(val) if val is not None else 'N/A'}"
                    for col, val in zip(columns, row)
                )
                lines.append(formatted_row)
            return _safe_join(lines)


        # --- Li·ªát k√™ chi ti·∫øt ---
        if is_list_query:
            row_limit = 1
            lines = []
            for row in rows[:row_limit]:
                formatted_row = "- " + ", ".join(
                    f"{col}: {str(val) if val is not None else 'N/A'}"
                    for col, val in zip(columns, row)
                )
                lines.append(formatted_row)

            if not filter_link and len(rows) > 2:
                    context_common = {"year": self._extract_year_from_question() or "2024"}
                    has_xuly = any(keyword in self.context['last_sql'].lower() for keyword in [
                        'ca_nhan', 'phong_ban', 'to_nhom', 'trung_tam'
                    ])
                    has_diachi = (
                        any(re.search(rf"\b{key}\b", self.context['last_sql'], re.IGNORECASE)
                            for key in ["TINH_THANH_PHO", "QUAN_HUYEN", "MA_TINH", "MA_HUYEN"]) or
                        "FB_LOCALITY" in self.context['last_sql'].upper()
                    )

                    if has_xuly:
                        context_form = context_common.copy()
                        # KH√îNG set c·ª©ng caNhan t·ª´ c√¢u h·ªèi n·ªØa, ƒë·ªÉ extract_form_detail_params_from_sql lo
                        params = extract_form_detail_params_from_sql(self.context['last_sql'], context_form)            
                        base_url = "http://14.160.91.174:8180/smartw/feedback/form/detail.htm"
                        filter_link = build_filter_url(base_url, params)
                    else:
                        base_url = "http://14.160.91.174:8180/smartw/feedback/list.htm"
                        ma_diachi = {}

                        ma_col_mapping = {
                            "tinhThanhPho": ["ma_tinh", "province", "tinh_thanh_pho"],
                            "quanHuyen": ["ma_huyen", "district", "quan_huyen"],
                        }

                        for key, aliases in ma_col_mapping.items():
                            for alias in aliases:
                                if alias in map(str.lower, columns):
                                    idx = next(i for i, col in enumerate(columns) if col.lower() == alias)
                                    ma_diachi[key] = rows[0][idx]
                                    logger.info(f"[DEBUG] D√πng tr·ª±c ti·∫øp m√£ {key} = {ma_diachi[key]} t·ª´ k·∫øt qu·∫£ truy v·∫•n")
                                    break

                        full_name_idx = next(
                            (i for i, col in enumerate(columns) if col.upper() in ["FULL_NAME", "DIA_DIEM_PHAN_ANH"]),
                            None
                        )

                        if not ma_diachi.get("tinhThanhPho") or not ma_diachi.get("quanHuyen"):
                            if full_name_idx is not None:
                                dia_chi = rows[0][full_name_idx]
                                logger.info(f"[DEBUG] ƒê·ªãa ch·ªâ trong k·∫øt qu·∫£ (full_name): {dia_chi}")
                                try:
                                    query = """
                                        SELECT province AS tinhThanhPho, district AS quanHuyen
                                        FROM FB_LOCALITY
                                        WHERE full_name = :1
                                        FETCH FIRST 1 ROWS ONLY
                                    """
                                    with self.db_conn.cursor() as cursor:
                                        cursor.execute(query, [dia_chi])
                                        result_fine = cursor.fetchone()
                                        if result_fine:
                                            col_names = [desc[0].lower() for desc in cursor.description]
                                            for idx, col in enumerate(col_names):
                                                if col in ["tinhthanhpho", "quanhuyen"]:
                                                    ma_diachi[col] = result_fine[idx]
                                            logger.info(f"[DEBUG] Mapping FULL_NAME ‚Üí m√£ ƒë·ªãa l√Ω: {ma_diachi}")
                                except Exception as e:
                                    logger.error(f"[ERROR] Truy v·∫•n m√£ ƒë·ªãa l√Ω t·ª´ FULL_NAME l·ªói: {e}")
                            else:
                                logger.warning("[DEBUG] Kh√¥ng t√¨m th·∫•y c·ªôt ƒë·ªãa ch·ªâ ph√π h·ª£p trong k·∫øt qu·∫£.")

                        if not ma_diachi.get("tinhThanhPho") or not ma_diachi.get("quanHuyen"):
                            dia_chi_cau_hoi = self._extract_dia_chi_from_question(self.context["last_question"] or "")
                            logger.info(f"[DEBUG] ƒê·ªãa ch·ªâ t·ª´ c√¢u h·ªèi: {dia_chi_cau_hoi}")
                            fuzzy_result = self.get_ma_dia_chi_fuzzy(dia_chi_cau_hoi)
                            if fuzzy_result:
                                logger.info(f"[DEBUG] Mapping ƒë·ªãa ch·ªâ t·ª´ c√¢u h·ªèi ‚Üí m√£ ƒë·ªãa l√Ω: {fuzzy_result}")
                                ma_diachi.update(fuzzy_result)

                        if ma_diachi:
                            context_common.update(ma_diachi)

                        if "tinhthanhpho" in context_common:
                            context_common["tinhThanhPho"] = context_common.pop("tinhthanhpho")
                        if "quanhuyen" in context_common:
                            context_common["quanHuyen"] = context_common.pop("quanhuyen")

                        params = extract_list_params_from_sql(self.context['last_sql'], context_common)
                        logger.info(f"[DEBUG] context_common: {context_common}")
                        logger.info(f"[DEBUG] params for build_filter_url: {params}")
                        filter_link = build_filter_url(base_url, params) 

            if filter_link:
                lines.append(f"\nüîó [Xem to√†n b·ªô danh s√°ch t·∫°i ƒë√¢y]({filter_link})")
            return _safe_join(lines)

        return "‚ö†Ô∏è Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c lo·∫°i c√¢u h·ªèi."
    def detect_query_type(self, question: str, result: dict):
        """X√°c ƒë·ªãnh lo·∫°i c√¢u h·ªèi: th·ªëng k√™, th√¥ng tin, li·ªát k√™"""
        q = (question or "").lower()

        is_list_query = any(kw in q for kw in [
            "li·ªát k√™", "danh s√°ch", "list", "show", "c√°c pa", 
            "c√°c ph·∫£n √°nh", "xem chi ti·∫øt", "chi ti·∫øt c√°c ph·∫£n √°nh", "pa ·ªü"
        ])
        is_statistical = any(kw in q for kw in [
            "theo t·ª´ng nh√≥m", "theo t·ª´ng lo·∫°i", "th·ªëng k√™", 
            "t·ª´ng nh√≥m", "t·ª´ng lo·∫°i", "m·ªói nh√≥m", "m·ªói lo·∫°i",
            "bao nhi√™u", "t·ªïng s·ªë", "t·ªïng c·ªông", "s·ªë l∆∞·ª£ng", 
            "bao nhi√™u ph·∫£n √°nh", "bn ph·∫£n √°nh", "bn pa", "bao nhi√™u pa"
        ]) or "group by" in q or "t·ª´ng nh√≥m" in q

        # X√°c ƒë·ªãnh c√≥ d·ªØ li·ªáu hay kh√¥ng
        rows = result.get("rows") or result.get("details", {}).get("rows")
        is_info_query = not is_list_query and not is_statistical and bool(rows)

        return is_statistical, is_info_query, is_list_query
    def response(self, question: str, result: dict, filter_link: str = None) -> str:
        self.context["last_question"] = question
        formatted_answer = self.format_result_for_user(result, filter_link)

        is_statistical, is_info_query, is_list_query = self.detect_query_type(
            question, result
        )

        if getattr(self, "groq_client", None) \
            and formatted_answer \
            and "Kh√¥ng c√≥ d·ªØ li·ªáu" not in formatted_answer \
            and (is_statistical or is_info_query):

            try:
                prompt = f"""
B·∫°n l√† tr·ª£ l√Ω tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n k·∫øt qu·∫£ truy v·∫•n.

C√¢u h·ªèi: {question}
K·∫øt qu·∫£ truy v·∫•n: {formatted_answer}

H∆Ø·ªöNG D·∫™N NGHI√äM NG·∫∂T:
- Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n d·ªØ li·ªáu trong "K·∫øt qu·∫£ truy v·∫•n".
- Ph·∫£n h·ªìi ph·∫£i gi·ªØ nguy√™n to√†n b·ªô gi√° tr·ªã trong {formatted_answer}, kh√¥ng ƒë∆∞·ª£c b·ªè b·ªõt ho·∫∑c suy lu·∫≠n th√™m.
- N·∫øu c√¢u h·ªèi y√™u c·∫ßu th·ªëng k√™ ho·∫∑c t·ªïng h·ª£p, ch·ªâ vi·ªác di·ªÖn ƒë·∫°t l·∫°i nh∆∞ng v·∫´n ph·∫£i n√™u r√µ s·ªë li·ªáu ch√≠nh x√°c t·ª´ {formatted_answer}.
- N·∫øu kh√¥ng ch·∫Øc ch·∫Øn, h√£y tr·∫£ nguy√™n {formatted_answer} m√† kh√¥ng thay ƒë·ªïi.
"""

                resp = self.groq_client.chat.completions.create(
                    model="llama-3.1-8b-instant",
                    messages=[
                        {"role": "system", "content": "B·∫°n l√† tr·ª£ l√Ω tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n k·∫øt qu·∫£ truy v·∫•n SQL."},
                        {"role": "user", "content": prompt}
                    ]
                )
                return resp.choices[0].message.content.strip()
            except Exception as e:
                import logging
                logging.error(f"[Groq ERROR] {e}")
                return formatted_answer

        return formatted_answer


    def clear_memory(self):
        if self.engine == "openai":
            self.memory.clear()
        else:
            self.memory = []
        logger.info("ƒê√£ x√≥a b·ªô nh·ªõ h·ªôi tho·∫°i.")
    def clear_cache(self):
        self.sql_cache.clear()
        self.schema_cache.clear()
        logger.info("ƒê√£ x√≥a SQL cache v√† schema cache.")

    def clear_all(self):
        self.clear_cache()
        self.clear_memory()
        self.last_context = {}
        logger.info("ƒê√£ x√≥a to√†n b·ªô b·ªô nh·ªõ v√† cache.")

    